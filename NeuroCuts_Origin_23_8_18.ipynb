{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "15x8RoDmvT7uwCijRN3ny_Yyy-62_DPOM",
      "authorship_tag": "ABX9TyNf+wzCgg7cq1vtAQOBuxhv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JiaChangGit/NeuroCuts_robin/blob/colab/NeuroCuts_Origin_23_8_18.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8OJdfsHdw53u",
        "outputId": "bc5c2ae6-9f45-40e5-85c0-5e926f8535cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#-----To install specific version 3.7 of python in Google Colab-----\n",
        "!sudo apt-get install python3.7\n",
        "!sudo apt-get update -y\n",
        "!sudo update-alternatives --install /usr/bin/python3 python3 /usr/bin/python3.7 1\n",
        "!sudo update-alternatives --config python3 #按有注音的數字鍵(選3.7)\n",
        "!python --version\n",
        "!sudo apt install python3-pip\n",
        "!sudo apt install python3.7-distutils\n",
        "!python -m pip install --upgrade pip\n",
        "!pip install tensorflow==1.14.0\n",
        "!pip install ray==0.7.6\n",
        "!pip install gym==0.17.3\n",
        "!pip install opencv-python\n",
        "!pip install setproctitle\n",
        "!pip install tabulate\n",
        "!pip install requests\n",
        "!pip install psutil\n",
        "!pip install lz4\n",
        "!pip uninstall protobuf\n",
        "!pip install protobuf==3.19.0\n",
        "!git clone https://github.com/JiaChangGit/NeuroCuts_robin\n",
        "%cd NeuroCuts_robin/\n",
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "t_Eebek-xLcY",
        "outputId": "8b4e3495-4f9e-43aa-93d8-8a7d41667990"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libpython3.7-minimal libpython3.7-stdlib mailcap mime-support\n",
            "  python3.7-minimal\n",
            "Suggested packages:\n",
            "  python3.7-venv binfmt-support\n",
            "The following NEW packages will be installed:\n",
            "  libpython3.7-minimal libpython3.7-stdlib mailcap mime-support python3.7\n",
            "  python3.7-minimal\n",
            "0 upgraded, 6 newly installed, 0 to remove and 16 not upgraded.\n",
            "Need to get 4,698 kB of archives.\n",
            "After this operation, 17.8 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 mailcap all 3.70+nmu1ubuntu1 [23.8 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 mime-support all 3.66 [3,696 B]\n",
            "Get:3 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.7-minimal amd64 3.7.17-1+jammy1 [608 kB]\n",
            "Get:4 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-minimal amd64 3.7.17-1+jammy1 [1,837 kB]\n",
            "Get:5 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 libpython3.7-stdlib amd64 3.7.17-1+jammy1 [1,864 kB]\n",
            "Get:6 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7 amd64 3.7.17-1+jammy1 [362 kB]\n",
            "Fetched 4,698 kB in 3s (1,442 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 6.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libpython3.7-minimal:amd64.\n",
            "(Reading database ... 120831 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libpython3.7-minimal_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.7-minimal:amd64 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3.7-minimal.\n",
            "Preparing to unpack .../1-python3.7-minimal_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.7-minimal (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package mailcap.\n",
            "Preparing to unpack .../2-mailcap_3.70+nmu1ubuntu1_all.deb ...\n",
            "Unpacking mailcap (3.70+nmu1ubuntu1) ...\n",
            "Selecting previously unselected package mime-support.\n",
            "Preparing to unpack .../3-mime-support_3.66_all.deb ...\n",
            "Unpacking mime-support (3.66) ...\n",
            "Selecting previously unselected package libpython3.7-stdlib:amd64.\n",
            "Preparing to unpack .../4-libpython3.7-stdlib_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking libpython3.7-stdlib:amd64 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3.7.\n",
            "Preparing to unpack .../5-python3.7_3.7.17-1+jammy1_amd64.deb ...\n",
            "Unpacking python3.7 (3.7.17-1+jammy1) ...\n",
            "Setting up libpython3.7-minimal:amd64 (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7-minimal (3.7.17-1+jammy1) ...\n",
            "Setting up mailcap (3.70+nmu1ubuntu1) ...\n",
            "Setting up mime-support (3.66) ...\n",
            "Setting up libpython3.7-stdlib:amd64 (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7 (3.7.17-1+jammy1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Get:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "Get:2 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [860 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [458 kB]\n",
            "Get:8 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [897 kB]\n",
            "Get:9 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [980 kB]\n",
            "Hit:10 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-backports InRelease [109 kB]\n",
            "Hit:12 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,136 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,241 kB]\n",
            "Hit:15 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [923 kB]\n",
            "Hit:17 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Fetched 6,838 kB in 1s (4,897 kB/s)\n",
            "Reading package lists... Done\n",
            "update-alternatives: using /usr/bin/python3.7 to provide /usr/bin/python3 (python3) in auto mode\n",
            "There is only one alternative in link group python3 (providing /usr/bin/python3): /usr/bin/python3.7\n",
            "Nothing to configure.\n",
            "Python 3.7.17\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3-setuptools python3-wheel\n",
            "Suggested packages:\n",
            "  python-setuptools-doc\n",
            "The following NEW packages will be installed:\n",
            "  python3-pip python3-setuptools python3-wheel\n",
            "0 upgraded, 3 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 1,677 kB of archives.\n",
            "After this operation, 8,965 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 python3-setuptools all 59.6.0-1.2ubuntu0.22.04.1 [339 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-wheel all 0.37.1-2ubuntu0.22.04.1 [32.0 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 python3-pip all 22.0.2+dfsg-1ubuntu0.3 [1,305 kB]\n",
            "Fetched 1,677 kB in 1s (2,087 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3-setuptools.\n",
            "(Reading database ... 121476 files and directories currently installed.)\n",
            "Preparing to unpack .../python3-setuptools_59.6.0-1.2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-wheel.\n",
            "Preparing to unpack .../python3-wheel_0.37.1-2ubuntu0.22.04.1_all.deb ...\n",
            "Unpacking python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package python3-pip.\n",
            "Preparing to unpack .../python3-pip_22.0.2+dfsg-1ubuntu0.3_all.deb ...\n",
            "Unpacking python3-pip (22.0.2+dfsg-1ubuntu0.3) ...\n",
            "Setting up python3-setuptools (59.6.0-1.2ubuntu0.22.04.1) ...\n",
            "Setting up python3-wheel (0.37.1-2ubuntu0.22.04.1) ...\n",
            "Setting up python3-pip (22.0.2+dfsg-1ubuntu0.3) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  python3.7-lib2to3\n",
            "The following NEW packages will be installed:\n",
            "  python3.7-distutils python3.7-lib2to3\n",
            "0 upgraded, 2 newly installed, 0 to remove and 20 not upgraded.\n",
            "Need to get 313 kB of archives.\n",
            "After this operation, 1,229 kB of additional disk space will be used.\n",
            "Get:1 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-lib2to3 all 3.7.17-1+jammy1 [124 kB]\n",
            "Get:2 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy/main amd64 python3.7-distutils all 3.7.17-1+jammy1 [189 kB]\n",
            "Fetched 313 kB in 1s (261 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 2.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package python3.7-lib2to3.\n",
            "(Reading database ... 122338 files and directories currently installed.)\n",
            "Preparing to unpack .../python3.7-lib2to3_3.7.17-1+jammy1_all.deb ...\n",
            "Unpacking python3.7-lib2to3 (3.7.17-1+jammy1) ...\n",
            "Selecting previously unselected package python3.7-distutils.\n",
            "Preparing to unpack .../python3.7-distutils_3.7.17-1+jammy1_all.deb ...\n",
            "Unpacking python3.7-distutils (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7-lib2to3 (3.7.17-1+jammy1) ...\n",
            "Setting up python3.7-distutils (3.7.17-1+jammy1) ...\n",
            "Requirement already satisfied: pip in /usr/lib/python3/dist-packages (22.0.2)\n",
            "Collecting pip\n",
            "  Downloading pip-23.2.1-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m36.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 22.0.2\n",
            "    Not uninstalling pip at /usr/lib/python3/dist-packages, outside environment /usr\n",
            "    Can't uninstall 'pip'. No files were found to uninstall.\n",
            "Successfully installed pip-23.2.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tensorflow==1.14.0\n",
            "  Downloading tensorflow-1.14.0-cp37-cp37m-manylinux1_x86_64.whl (109.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.3/109.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting absl-py>=0.7.0 (from tensorflow==1.14.0)\n",
            "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting astor>=0.6.0 (from tensorflow==1.14.0)\n",
            "  Downloading astor-0.8.1-py2.py3-none-any.whl (27 kB)\n",
            "Collecting gast>=0.2.0 (from tensorflow==1.14.0)\n",
            "  Downloading gast-0.5.4-py3-none-any.whl (19 kB)\n",
            "Collecting google-pasta>=0.1.6 (from tensorflow==1.14.0)\n",
            "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-applications>=1.0.6 (from tensorflow==1.14.0)\n",
            "  Downloading Keras_Applications-1.0.8-py3-none-any.whl (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.7/50.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras-preprocessing>=1.0.5 (from tensorflow==1.14.0)\n",
            "  Downloading Keras_Preprocessing-1.1.2-py2.py3-none-any.whl (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.6/42.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy<2.0,>=1.14.5 (from tensorflow==1.14.0)\n",
            "  Downloading numpy-1.21.6-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.7/15.7 MB\u001b[0m \u001b[31m74.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/lib/python3/dist-packages (from tensorflow==1.14.0) (1.16.0)\n",
            "Collecting protobuf>=3.6.1 (from tensorflow==1.14.0)\n",
            "  Obtaining dependency information for protobuf>=3.6.1 from https://files.pythonhosted.org/packages/ed/33/f7a5717125d9f699b193fa23904725514b82643d522aa189aba03149ba3b/protobuf-4.24.0-cp37-abi3-manylinux2014_x86_64.whl.metadata\n",
            "  Downloading protobuf-4.24.0-cp37-abi3-manylinux2014_x86_64.whl.metadata (540 bytes)\n",
            "Collecting tensorboard<1.15.0,>=1.14.0 (from tensorflow==1.14.0)\n",
            "  Downloading tensorboard-1.14.0-py3-none-any.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m80.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 (from tensorflow==1.14.0)\n",
            "  Downloading tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m488.5/488.5 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting termcolor>=1.1.0 (from tensorflow==1.14.0)\n",
            "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
            "Collecting wrapt>=1.11.1 (from tensorflow==1.14.0)\n",
            "  Downloading wrapt-1.15.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.7/75.7 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting grpcio>=1.8.6 (from tensorflow==1.14.0)\n",
            "  Obtaining dependency information for grpcio>=1.8.6 from https://files.pythonhosted.org/packages/ca/fb/a23f72166bc650652e93500cf596f724dbe9468cd1e74cc2e546d6a4fe6e/grpcio-1.57.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading grpcio-1.57.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/lib/python3/dist-packages (from tensorflow==1.14.0) (0.37.1)\n",
            "Collecting h5py (from keras-applications>=1.0.6->tensorflow==1.14.0)\n",
            "  Downloading h5py-3.8.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.3/4.3 MB\u001b[0m \u001b[31m69.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting markdown>=2.6.8 (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0)\n",
            "  Obtaining dependency information for markdown>=2.6.8 from https://files.pythonhosted.org/packages/1a/b5/228c1cdcfe138f1a8e01ab1b54284c8b83735476cb22b6ba251656ed13ad/Markdown-3.4.4-py3-none-any.whl.metadata\n",
            "  Downloading Markdown-3.4.4-py3-none-any.whl.metadata (6.9 kB)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/lib/python3/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (59.6.0)\n",
            "Collecting werkzeug>=0.11.15 (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0)\n",
            "  Downloading Werkzeug-2.2.3-py3-none-any.whl (233 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m233.6/233.6 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-metadata>=4.4 (from markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0)\n",
            "  Obtaining dependency information for importlib-metadata>=4.4 from https://files.pythonhosted.org/packages/ff/94/64287b38c7de4c90683630338cf28f129decbba0a44f0c6db35a873c73c4/importlib_metadata-6.7.0-py3-none-any.whl.metadata\n",
            "  Downloading importlib_metadata-6.7.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting MarkupSafe>=2.1.1 (from werkzeug>=0.11.15->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0)\n",
            "  Obtaining dependency information for MarkupSafe>=2.1.1 from https://files.pythonhosted.org/packages/e5/dd/49576e803c0d974671e44fa78049217fcc68af3662a24f831525ed30e6c7/MarkupSafe-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading MarkupSafe-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Collecting zipp>=0.5 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0)\n",
            "  Downloading zipp-3.15.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting typing-extensions>=3.6.4 (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0)\n",
            "  Obtaining dependency information for typing-extensions>=3.6.4 from https://files.pythonhosted.org/packages/ec/6b/63cc3df74987c36fe26157ee12e09e8f9db4de771e0f3404263117e75b95/typing_extensions-4.7.1-py3-none-any.whl.metadata\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl.metadata (3.1 kB)\n",
            "Downloading grpcio-1.57.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.3/5.3 MB\u001b[0m \u001b[31m117.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-4.24.0-cp37-abi3-manylinux2014_x86_64.whl (311 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m311.6/311.6 kB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Markdown-3.4.4-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.2/94.2 kB\u001b[0m \u001b[31m12.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading importlib_metadata-6.7.0-py3-none-any.whl (22 kB)\n",
            "Downloading MarkupSafe-2.1.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (25 kB)\n",
            "Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Installing collected packages: tensorflow-estimator, zipp, wrapt, typing-extensions, termcolor, protobuf, numpy, MarkupSafe, grpcio, google-pasta, gast, astor, absl-py, werkzeug, keras-preprocessing, importlib-metadata, h5py, markdown, keras-applications, tensorboard, tensorflow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "launchpadlib 1.10.16 requires httplib2, which is not installed.\n",
            "lazr-restfulclient 0.14.4 requires httplib2>=0.7.7, which is not installed.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-2.1.3 absl-py-1.4.0 astor-0.8.1 gast-0.5.4 google-pasta-0.2.0 grpcio-1.57.0 h5py-3.8.0 importlib-metadata-6.7.0 keras-applications-1.0.8 keras-preprocessing-1.1.2 markdown-3.4.4 numpy-1.21.6 protobuf-4.24.0 tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0 termcolor-2.3.0 typing-extensions-4.7.1 werkzeug-2.2.3 wrapt-1.15.0 zipp-3.15.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting ray==0.7.6\n",
            "  Downloading ray-0.7.6-cp37-cp37m-manylinux1_x86_64.whl (75.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.8/75.8 MB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14 in /usr/local/lib/python3.7/dist-packages (from ray==0.7.6) (1.21.6)\n",
            "Collecting filelock (from ray==0.7.6)\n",
            "  Obtaining dependency information for filelock from https://files.pythonhosted.org/packages/00/45/ec3407adf6f6b5bf867a4462b2b0af27597a26bd3cd6e2534cb6ab029938/filelock-3.12.2-py3-none-any.whl.metadata\n",
            "  Downloading filelock-3.12.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Collecting jsonschema (from ray==0.7.6)\n",
            "  Downloading jsonschema-4.17.3-py3-none-any.whl (90 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.4/90.4 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting funcsigs (from ray==0.7.6)\n",
            "  Downloading funcsigs-1.0.2-py2.py3-none-any.whl (17 kB)\n",
            "Collecting click (from ray==0.7.6)\n",
            "  Obtaining dependency information for click from https://files.pythonhosted.org/packages/00/2e/d53fa4befbf2cfa713304affc7ca780ce4fc1fd8710527771b58311a3229/click-8.1.7-py3-none-any.whl.metadata\n",
            "  Downloading click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting colorama (from ray==0.7.6)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Collecting pytest (from ray==0.7.6)\n",
            "  Obtaining dependency information for pytest from https://files.pythonhosted.org/packages/33/b2/741130cbcf2bbfa852ed95a60dc311c9e232c7ed25bac3d9b8880a8df4ae/pytest-7.4.0-py3-none-any.whl.metadata\n",
            "  Downloading pytest-7.4.0-py3-none-any.whl.metadata (8.0 kB)\n",
            "Collecting pyyaml (from ray==0.7.6)\n",
            "  Obtaining dependency information for pyyaml from https://files.pythonhosted.org/packages/d7/8f/db62b0df635b9008fe90aa68424e99cee05e68b398740c8a666a98455589/PyYAML-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading PyYAML-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting redis>=3.3.2 (from ray==0.7.6)\n",
            "  Obtaining dependency information for redis>=3.3.2 from https://files.pythonhosted.org/packages/df/b2/dfdc17f701f7b587f6c89c2b9b6b5978c87a8a785555efc810b064c875de/redis-5.0.0-py3-none-any.whl.metadata\n",
            "  Downloading redis-5.0.0-py3-none-any.whl.metadata (8.8 kB)\n",
            "Requirement already satisfied: six>=1.0.0 in /usr/lib/python3/dist-packages (from ray==0.7.6) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.8.0 in /usr/local/lib/python3.7/dist-packages (from ray==0.7.6) (4.24.0)\n",
            "Collecting async-timeout>=4.0.2 (from redis>=3.3.2->ray==0.7.6)\n",
            "  Obtaining dependency information for async-timeout>=4.0.2 from https://files.pythonhosted.org/packages/a7/fa/e01228c2938de91d47b307831c62ab9e4001e747789d0b05baf779a6488c/async_timeout-4.0.3-py3-none-any.whl.metadata\n",
            "  Downloading async_timeout-4.0.3-py3-none-any.whl.metadata (4.2 kB)\n",
            "Requirement already satisfied: importlib-metadata>=1.0 in /usr/local/lib/python3.7/dist-packages (from redis>=3.3.2->ray==0.7.6) (6.7.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from redis>=3.3.2->ray==0.7.6) (4.7.1)\n",
            "Collecting attrs>=17.4.0 (from jsonschema->ray==0.7.6)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting importlib-resources>=1.4.0 (from jsonschema->ray==0.7.6)\n",
            "  Downloading importlib_resources-5.12.0-py3-none-any.whl (36 kB)\n",
            "Collecting pkgutil-resolve-name>=1.3.10 (from jsonschema->ray==0.7.6)\n",
            "  Downloading pkgutil_resolve_name-1.3.10-py3-none-any.whl (4.7 kB)\n",
            "Collecting pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 (from jsonschema->ray==0.7.6)\n",
            "  Downloading pyrsistent-0.19.3-py3-none-any.whl (57 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m6.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting iniconfig (from pytest->ray==0.7.6)\n",
            "  Downloading iniconfig-2.0.0-py3-none-any.whl (5.9 kB)\n",
            "Collecting packaging (from pytest->ray==0.7.6)\n",
            "  Downloading packaging-23.1-py3-none-any.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.9/48.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pluggy<2.0,>=0.12 (from pytest->ray==0.7.6)\n",
            "  Obtaining dependency information for pluggy<2.0,>=0.12 from https://files.pythonhosted.org/packages/51/32/4a79112b8b87b21450b066e102d6608907f4c885ed7b04c3fdb085d4d6ae/pluggy-1.2.0-py3-none-any.whl.metadata\n",
            "  Downloading pluggy-1.2.0-py3-none-any.whl.metadata (4.4 kB)\n",
            "Collecting exceptiongroup>=1.0.0rc8 (from pytest->ray==0.7.6)\n",
            "  Obtaining dependency information for exceptiongroup>=1.0.0rc8 from https://files.pythonhosted.org/packages/ad/83/b71e58666f156a39fb29417e4c8ca4bc7400c0dd4ed9e8842ab54dc8c344/exceptiongroup-1.1.3-py3-none-any.whl.metadata\n",
            "  Downloading exceptiongroup-1.1.3-py3-none-any.whl.metadata (6.1 kB)\n",
            "Collecting tomli>=1.0.0 (from pytest->ray==0.7.6)\n",
            "  Downloading tomli-2.0.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=1.0->redis>=3.3.2->ray==0.7.6) (3.15.0)\n",
            "Downloading redis-5.0.0-py3-none-any.whl (250 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m250.1/250.1 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.7-py3-none-any.whl (97 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.9/97.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading filelock-3.12.2-py3-none-any.whl (10 kB)\n",
            "Downloading pytest-7.4.0-py3-none-any.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.6/323.6 kB\u001b[0m \u001b[31m30.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading PyYAML-6.0.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (670 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.1/670.1 kB\u001b[0m \u001b[31m45.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading async_timeout-4.0.3-py3-none-any.whl (5.7 kB)\n",
            "Downloading exceptiongroup-1.1.3-py3-none-any.whl (14 kB)\n",
            "Downloading pluggy-1.2.0-py3-none-any.whl (17 kB)\n",
            "Installing collected packages: funcsigs, tomli, pyyaml, pyrsistent, pkgutil-resolve-name, packaging, iniconfig, importlib-resources, filelock, exceptiongroup, colorama, async-timeout, redis, pluggy, click, attrs, pytest, jsonschema, ray\n",
            "Successfully installed async-timeout-4.0.3 attrs-23.1.0 click-8.1.7 colorama-0.4.6 exceptiongroup-1.1.3 filelock-3.12.2 funcsigs-1.0.2 importlib-resources-5.12.0 iniconfig-2.0.0 jsonschema-4.17.3 packaging-23.1 pkgutil-resolve-name-1.3.10 pluggy-1.2.0 pyrsistent-0.19.3 pytest-7.4.0 pyyaml-6.0.1 ray-0.7.6 redis-5.0.0 tomli-2.0.1\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting gym==0.17.3\n",
            "  Downloading gym-0.17.3.tar.gz (1.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting cloudpickle<1.7.0,>=1.2.0 (from gym==0.17.3)\n",
            "  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym==0.17.3) (1.21.6)\n",
            "Collecting pyglet<=1.5.0,>=1.4.0 (from gym==0.17.3)\n",
            "  Downloading pyglet-1.5.0-py2.py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy (from gym==0.17.3)\n",
            "  Downloading scipy-1.7.3-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (38.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.1/38.1 MB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting future (from pyglet<=1.5.0,>=1.4.0->gym==0.17.3)\n",
            "  Downloading future-0.18.3.tar.gz (840 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m840.9/840.9 kB\u001b[0m \u001b[31m58.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gym, future\n",
            "  Building wheel for gym (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.17.3-py3-none-any.whl size=1654653 sha256=be3ee2bc17e9f2fd2c02295a7bd8df74074012585e3af93aa5d40a3ffb7f2d8e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d1/81/4b/dd9c029691022cb957398d1f015e66b75e37637dda61abdf58\n",
            "  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for future: filename=future-0.18.3-py3-none-any.whl size=492037 sha256=8117aa4d105f4c45699ef1a1aad7efdde4c86e27e8a228134b7ac6f844fccaed\n",
            "  Stored in directory: /root/.cache/pip/wheels/fa/cd/1f/c6b7b50b564983bf3011e8fc75d06047ddc50c07f6e3660b00\n",
            "Successfully built gym future\n",
            "Installing collected packages: scipy, future, cloudpickle, pyglet, gym\n",
            "Successfully installed cloudpickle-1.6.0 future-0.18.3 gym-0.17.3 pyglet-1.5.0 scipy-1.7.3\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting opencv-python\n",
            "  Obtaining dependency information for opencv-python from https://files.pythonhosted.org/packages/f5/d0/2e455d894ec0d6527e662ad55e70c04f421ad83a6fd0a54c3dd73c411282/opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (19 kB)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.7/dist-packages (from opencv-python) (1.21.6)\n",
            "Downloading opencv_python-4.8.0.76-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python\n",
            "Successfully installed opencv-python-4.8.0.76\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting setproctitle\n",
            "  Downloading setproctitle-1.3.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Installing collected packages: setproctitle\n",
            "Successfully installed setproctitle-1.3.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting tabulate\n",
            "  Downloading tabulate-0.9.0-py3-none-any.whl (35 kB)\n",
            "Installing collected packages: tabulate\n",
            "Successfully installed tabulate-0.9.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting requests\n",
            "  Obtaining dependency information for requests from https://files.pythonhosted.org/packages/70/8e/0e2d847013cb52cd35b38c009bb167a1a26b2ce6cd6965bf26b47bc0bf44/requests-2.31.0-py3-none-any.whl.metadata\n",
            "  Downloading requests-2.31.0-py3-none-any.whl.metadata (4.6 kB)\n",
            "Collecting charset-normalizer<4,>=2 (from requests)\n",
            "  Obtaining dependency information for charset-normalizer<4,>=2 from https://files.pythonhosted.org/packages/89/f5/88e9dd454756fea555198ddbe6fa40d6408ec4f10ad4f0a911e0b7e471e4/charset_normalizer-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n",
            "  Downloading charset_normalizer-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (31 kB)\n",
            "Collecting idna<4,>=2.5 (from requests)\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests)\n",
            "  Obtaining dependency information for urllib3<3,>=1.21.1 from https://files.pythonhosted.org/packages/9b/81/62fd61001fa4b9d0df6e31d47ff49cfa9de4af03adecf339c7bc30656b37/urllib3-2.0.4-py3-none-any.whl.metadata\n",
            "  Downloading urllib3-2.0.4-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests)\n",
            "  Obtaining dependency information for certifi>=2017.4.17 from https://files.pythonhosted.org/packages/4c/dd/2234eab22353ffc7d94e8d13177aaa050113286e93e7b40eae01fbf7c3d9/certifi-2023.7.22-py3-none-any.whl.metadata\n",
            "  Downloading certifi-2023.7.22-py3-none-any.whl.metadata (2.2 kB)\n",
            "Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading certifi-2023.7.22-py3-none-any.whl (158 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.3/158.3 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.2.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (175 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m175.8/175.8 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.0.4-py3-none-any.whl (123 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.9/123.9 kB\u001b[0m \u001b[31m13.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: urllib3, idna, charset-normalizer, certifi, requests\n",
            "Successfully installed certifi-2023.7.22 charset-normalizer-3.2.0 idna-3.4 requests-2.31.0 urllib3-2.0.4\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting psutil\n",
            "  Downloading psutil-5.9.5-cp36-abi3-manylinux_2_12_x86_64.manylinux2010_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (282 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/282.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m \u001b[32m276.5/282.1 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m282.1/282.1 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: psutil\n",
            "Successfully installed psutil-5.9.5\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "psutil"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lz4\n",
            "  Downloading lz4-4.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m42.7 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lz4\n",
            "Successfully installed lz4-4.3.2\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mFound existing installation: protobuf 4.24.0\n",
            "Uninstalling protobuf-4.24.0:\n",
            "  Would remove:\n",
            "    /usr/local/lib/python3.7/dist-packages/google/_upb/_message.abi3.so\n",
            "    /usr/local/lib/python3.7/dist-packages/google/protobuf/*\n",
            "    /usr/local/lib/python3.7/dist-packages/protobuf-4.24.0.dist-info/*\n",
            "Proceed (Y/n)? y\n",
            "  Successfully uninstalled protobuf-4.24.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting protobuf==3.19.0\n",
            "  Downloading protobuf-3.19.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: protobuf\n",
            "Successfully installed protobuf-3.19.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
            "\u001b[0mCloning into 'NeuroCuts_robin'...\n",
            "remote: Enumerating objects: 77, done.\u001b[K\n",
            "remote: Counting objects: 100% (77/77), done.\u001b[K\n",
            "remote: Compressing objects: 100% (44/44), done.\u001b[K\n",
            "remote: Total 77 (delta 34), reused 75 (delta 32), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (77/77), 26.68 MiB | 29.76 MiB/s, done.\n",
            "Resolving deltas: 100% (34/34), done.\n",
            "/content/NeuroCuts_robin\n",
            "\u001b[0m\u001b[01;34mclassbench\u001b[0m/  hicuts.py        neurocuts_env.py   run_neurocuts.py\n",
            "\u001b[01;34mcontext\u001b[0m/     hypercuts.py     pickle-to-json.py  tensorboard.png\n",
            "cutsplit.py  inspect_tree.py  README.md          test.py\n",
            "Dockerfile   LICENSE          \u001b[01;34mruleset\u001b[0m/           tree.py\n",
            "efficuts.py  mask.py          run_baselines.py   Visualize.ipynb\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example\n",
        "!python run_neurocuts.py --rules=fw1_1k  --dump-dir=/content/drive/MyDrive/CIAL/neurocut_update_efficut/w=1,u=0/fw1_1k --depth-weight=1 --partition-mode=efficuts"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ttu5rnylygcI",
        "outputId": "2e7f16cb-95ba-40a6-aaf3-144458cc5014"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "2023-08-18 03:58:04,518\tINFO resource_spec.py:205 -- Starting Ray with 6.88 GiB memory available for workers and up to 3.46 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2023-08-18 03:58:04,907\tWARNING sample.py:31 -- DeprecationWarning: wrapping <function on_episode_end at 0x7ecb821960e0> with tune.function() is no longer needed\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/6.88 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 1.2/12.7 GiB\n",
            "\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.88 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 1.2/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_efficuts\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw1_1k,tree_gae_lambda=0.95:\tRUNNING\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m 2023-08-18 03:58:08,329\tINFO trainer.py:345 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m Split not cached, recomputing\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad5701a7d90> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m Saving tree to /content/drive/MyDrive/CIAL/neurocut_update_efficut/w=1,u=0/fw1_1k/fw1_1k-111-acc-1157-uacc-96.98-bytes-1692331107.2339785.pkl\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56d901f10> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad5700710d0> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m Saving tree to /content/drive/MyDrive/CIAL/neurocut_update_efficut/w=1,u=0/fw1_1k/fw1_1k-95-acc-650-uacc-66.22-bytes-1692331165.1275017.pkl\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56b1b5ed0> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m Saving tree to /content/drive/MyDrive/CIAL/neurocut_update_efficut/w=1,u=0/fw1_1k/fw1_1k-112-acc-311-uacc-42.65-bytes-1692331168.3701735.pkl\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56d5e7fd0> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m Saving tree to /content/drive/MyDrive/CIAL/neurocut_update_efficut/w=1,u=0/fw1_1k/fw1_1k-88-acc-174-uacc-38.16-bytes-1692331172.0691142.pkl\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56c44fa10> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56d901f10> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56c4f8cd0> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56c445e90> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56d0c82d0> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56c1dff10> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56bc85610> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56d0af610> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56c5498d0> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56c9d5e90> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56c5bb090> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad573851150> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56a535290> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56bd43a90> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad569dc6950> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "\u001b[2m\u001b[36m(pid=3595)\u001b[0m <efficuts.EffiCuts object at 0x7ad56b5030d0> [431, 129, 85, 71, 48, 31, 22, 9, 9, 7, 6, 5, 4]\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example\n",
        "!python run_neurocuts.py --rules=fw2_1k  --dump-dir=/content/drive/MyDrive/CIAL/neurocut_update/w=1,u=0/fw2_1k --depth-weight=1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOiDV0h70JE3",
        "outputId": "1e77c9e2-048e-4738-b2c1-587ab7d5940b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow/python/compat/v2_compat.py:61: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "non-resource variables are not supported in the long term\n",
            "2023-08-18 04:19:30,458\tINFO resource_spec.py:205 -- Starting Ray with 6.84 GiB memory available for workers and up to 3.44 GiB for objects. You can adjust these settings with ray.init(memory=<bytes>, object_store_memory=<bytes>).\n",
            "2023-08-18 04:19:30,877\tWARNING sample.py:31 -- DeprecationWarning: wrapping <function on_episode_end at 0x7bae52b97170> with tune.function() is no longer needed\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 0/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 1.3/12.7 GiB\n",
            "\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 1.3/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m /usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m   np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:19:33,243\tINFO trainer.py:345 -- Tip: set 'eager': true or the --eager flag to enable TensorFlow eager execution\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 2483.7590113285273\n",
            "    bytes_per_rule_mean: 2483.7590113285273\n",
            "    bytes_per_rule_min: 2483.7590113285273\n",
            "    bytes_per_rule_valid_max: .nan\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: .nan\n",
            "    exceeded_max_depth_max: 12050\n",
            "    exceeded_max_depth_mean: 12050.0\n",
            "    exceeded_max_depth_min: 12050\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 501.0\n",
            "    memory_access_min: 501\n",
            "    memory_access_valid_max: .nan\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: .nan\n",
            "    nodes_remaining_max: 12579\n",
            "    nodes_remaining_mean: 12579.0\n",
            "    nodes_remaining_min: 12579\n",
            "    num_nodes_max: 61555\n",
            "    num_nodes_mean: 61555.0\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: .nan\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: .nan\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 15001.0\n",
            "    num_splits_min: 15001\n",
            "    num_splits_valid_max: .nan\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: .nan\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 971.0\n",
            "    rules_remaining_min: 971\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 501.0\n",
            "    tree_depth_min: 501\n",
            "    tree_depth_valid_max: .nan\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: .nan\n",
            "    update_memory_access_max: 46200\n",
            "    update_memory_access_mean: 46200.0\n",
            "    update_memory_access_min: 46200\n",
            "  date: 2023-08-18_04-22-00\n",
            "  done: false\n",
            "  episode_len_mean: 15001.0\n",
            "  episode_reward_max: -166334.0\n",
            "  episode_reward_mean: -166334.0\n",
            "  episode_reward_min: -166334.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 1\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 34961.528\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 3.1999082565307617\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.018844565376639366\n",
            "        policy_loss: -0.02298557572066784\n",
            "        total_loss: 2682.95458984375\n",
            "        vf_explained_var: 6.159146437312302e-07\n",
            "        vf_loss: 2683.005615234375\n",
            "    load_time_ms: 160.334\n",
            "    num_steps_sampled: 15001\n",
            "    num_steps_trained: 15000\n",
            "    sample_time_ms: 103784.91\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 1\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 3.5847412905777603\n",
            "    mean_inference_ms: 1.019324354927725\n",
            "    mean_processing_ms: 2.308188605476993\n",
            "  time_since_restore: 138.97542023658752\n",
            "  time_this_iter_s: 138.97542023658752\n",
            "  time_total_s: 138.97542023658752\n",
            "  timestamp: 1692332520\n",
            "  timesteps_since_restore: 15001\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 15001\n",
            "  training_iteration: 1\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.1/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 138 s, 1 iter, 15001 ts, -1.66e+05 rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:22:00,284\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 16633.0x the scale of `vf_clip_param`. This means that it will take more than 16633.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 2483.7590113285273\n",
            "    bytes_per_rule_mean: 2030.3604531410915\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: .nan\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: .nan\n",
            "    exceeded_max_depth_max: 12050\n",
            "    exceeded_max_depth_mean: 6025.0\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 501.0\n",
            "    memory_access_min: 501\n",
            "    memory_access_valid_max: .nan\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: .nan\n",
            "    nodes_remaining_max: 12579\n",
            "    nodes_remaining_mean: 6334.5\n",
            "    nodes_remaining_min: 90\n",
            "    num_nodes_max: 160083\n",
            "    num_nodes_mean: 110819.0\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: .nan\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: .nan\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 15001.0\n",
            "    num_splits_min: 15001\n",
            "    num_splits_valid_max: .nan\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: .nan\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 971.0\n",
            "    rules_remaining_min: 971\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 501.0\n",
            "    tree_depth_min: 501\n",
            "    tree_depth_valid_max: .nan\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: .nan\n",
            "    update_memory_access_max: 46200\n",
            "    update_memory_access_mean: 34313.0\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_04-25-09\n",
            "  done: false\n",
            "  episode_len_mean: 15001.0\n",
            "  episode_reward_max: -166334.0\n",
            "  episode_reward_mean: -303633.0\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 2\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 35710.148\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 0.20000000298023224\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 3.0523223876953125\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.032463107258081436\n",
            "        policy_loss: -0.04445606470108032\n",
            "        total_loss: 4459.11181640625\n",
            "        vf_explained_var: 6.596247317247617e-07\n",
            "        vf_loss: 4459.1796875\n",
            "    load_time_ms: 87.702\n",
            "    num_steps_sampled: 30002\n",
            "    num_steps_trained: 30000\n",
            "    sample_time_ms: 128041.409\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 2\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 4.738512416425436\n",
            "    mean_inference_ms: 1.0663591060566784\n",
            "    mean_processing_ms: 1.916031766879859\n",
            "  time_since_restore: 327.76431250572205\n",
            "  time_this_iter_s: 188.78889226913452\n",
            "  time_total_s: 327.76431250572205\n",
            "  timestamp: 1692332709\n",
            "  timesteps_since_restore: 30002\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 30002\n",
            "  training_iteration: 2\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.3/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 327 s, 2 iter, 30002 ts, -3.04e+05 rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:25:09,097\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 30363.0x the scale of `vf_clip_param`. This means that it will take more than 30363.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 2483.7590113285273\n",
            "    bytes_per_rule_mean: 2076.9220734637834\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: .nan\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: .nan\n",
            "    exceeded_max_depth_max: 16706\n",
            "    exceeded_max_depth_mean: 9585.333333333334\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 501.0\n",
            "    memory_access_min: 501\n",
            "    memory_access_valid_max: .nan\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: .nan\n",
            "    nodes_remaining_max: 17160\n",
            "    nodes_remaining_mean: 9943.0\n",
            "    nodes_remaining_min: 90\n",
            "    num_nodes_max: 160083\n",
            "    num_nodes_mean: 98717.66666666667\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: .nan\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: .nan\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 15001.0\n",
            "    num_splits_min: 15001\n",
            "    num_splits_valid_max: .nan\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: .nan\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 971.0\n",
            "    rules_remaining_min: 971\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 501.0\n",
            "    tree_depth_min: 501\n",
            "    tree_depth_valid_max: .nan\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: .nan\n",
            "    update_memory_access_max: 59092\n",
            "    update_memory_access_mean: 42572.666666666664\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_04-27-09\n",
            "  done: false\n",
            "  episode_len_mean: 15001.0\n",
            "  episode_reward_max: -161261.0\n",
            "  episode_reward_mean: -256175.66666666666\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 3\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 35002.933\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 3.121692419052124\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.018462048843503\n",
            "        policy_loss: -0.02716950885951519\n",
            "        total_loss: 2661.77490234375\n",
            "        vf_explained_var: 0.007720244117081165\n",
            "        vf_loss: 2661.827880859375\n",
            "    load_time_ms: 65.392\n",
            "    num_steps_sampled: 45003\n",
            "    num_steps_trained: 45000\n",
            "    sample_time_ms: 114333.684\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 3\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 4.907549682251908\n",
            "    mean_inference_ms: 1.0735657100324012\n",
            "    mean_processing_ms: 1.704831771004019\n",
            "  time_since_restore: 448.3266191482544\n",
            "  time_this_iter_s: 120.56230664253235\n",
            "  time_total_s: 448.3266191482544\n",
            "  timestamp: 1692332829\n",
            "  timesteps_since_restore: 45003\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 45003\n",
            "  training_iteration: 3\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.3/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 448 s, 3 iter, 45003 ts, -2.56e+05 rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:27:09,681\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 25618.0x the scale of `vf_clip_param`. This means that it will take more than 25618.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 3581.2914521112257\n",
            "    bytes_per_rule_mean: 2453.0144181256437\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: .nan\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: .nan\n",
            "    exceeded_max_depth_max: 16706\n",
            "    exceeded_max_depth_mean: 10499.5\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 501.0\n",
            "    memory_access_min: 501\n",
            "    memory_access_valid_max: .nan\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: .nan\n",
            "    nodes_remaining_max: 17160\n",
            "    nodes_remaining_mean: 10888.75\n",
            "    nodes_remaining_min: 90\n",
            "    num_nodes_max: 160083\n",
            "    num_nodes_mean: 96917.0\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: .nan\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: .nan\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 15001.0\n",
            "    num_splits_min: 15001\n",
            "    num_splits_valid_max: .nan\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: .nan\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 971.0\n",
            "    rules_remaining_min: 971\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 501.0\n",
            "    tree_depth_min: 501\n",
            "    tree_depth_valid_max: .nan\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: .nan\n",
            "    update_memory_access_max: 65268\n",
            "    update_memory_access_mean: 48246.5\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_04-29-24\n",
            "  done: false\n",
            "  episode_len_mean: 15001.0\n",
            "  episode_reward_max: -161261.0\n",
            "  episode_reward_mean: -234133.75\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 4\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 34982.211\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 0.30000001192092896\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 3.0241920948028564\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.021825868636369705\n",
            "        policy_loss: -0.03420177847146988\n",
            "        total_loss: 2662.151123046875\n",
            "        vf_explained_var: 0.006212695501744747\n",
            "        vf_loss: 2662.208740234375\n",
            "    load_time_ms: 53.316\n",
            "    num_steps_sampled: 60004\n",
            "    num_steps_trained: 60000\n",
            "    sample_time_ms: 110707.876\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 4\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 4.967500114899001\n",
            "    mean_inference_ms: 1.0688630774755925\n",
            "    mean_processing_ms: 1.5717498657769262\n",
            "  time_since_restore: 583.123053073883\n",
            "  time_this_iter_s: 134.79643392562866\n",
            "  time_total_s: 583.123053073883\n",
            "  timestamp: 1692332964\n",
            "  timesteps_since_restore: 60004\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 60004\n",
            "  training_iteration: 4\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:29:24,494\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 23413.0x the scale of `vf_clip_param`. This means that it will take more than 23413.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.3/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 583 s, 4 iter, 60004 ts, -2.34e+05 rew\n",
            "\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 3581.2914521112257\n",
            "    bytes_per_rule_mean: 2549.6626158599383\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: .nan\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: .nan\n",
            "    exceeded_max_depth_max: 17088\n",
            "    exceeded_max_depth_mean: 11817.2\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 501.0\n",
            "    memory_access_min: 501\n",
            "    memory_access_valid_max: .nan\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: .nan\n",
            "    nodes_remaining_max: 17586\n",
            "    nodes_remaining_mean: 12228.2\n",
            "    nodes_remaining_min: 90\n",
            "    num_nodes_max: 160083\n",
            "    num_nodes_mean: 92868.6\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: .nan\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: .nan\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 15001.0\n",
            "    num_splits_min: 15001\n",
            "    num_splits_valid_max: .nan\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: .nan\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 971.0\n",
            "    rules_remaining_min: 971\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 501.0\n",
            "    tree_depth_min: 501\n",
            "    tree_depth_valid_max: .nan\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: .nan\n",
            "    update_memory_access_max: 65268\n",
            "    update_memory_access_mean: 50249.4\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_04-31-27\n",
            "  done: false\n",
            "  episode_len_mean: 15001.0\n",
            "  episode_reward_max: -161261.0\n",
            "  episode_reward_mean: -219932.2\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 5\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 35046.44\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 3.0876657962799072\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.015827089548110962\n",
            "        policy_loss: -0.02425968460738659\n",
            "        total_loss: 2653.375732421875\n",
            "        vf_explained_var: 0.011098106391727924\n",
            "        vf_loss: 2653.423583984375\n",
            "    load_time_ms: 46.053\n",
            "    num_steps_sampled: 75005\n",
            "    num_steps_trained: 75000\n",
            "    sample_time_ms: 106097.406\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 5\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 4.958127638291215\n",
            "    mean_inference_ms: 1.0593954282622893\n",
            "    mean_processing_ms: 1.4824605137687565\n",
            "  time_since_restore: 706.1297793388367\n",
            "  time_this_iter_s: 123.00672626495361\n",
            "  time_total_s: 706.1297793388367\n",
            "  timestamp: 1692333087\n",
            "  timesteps_since_restore: 75005\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 75005\n",
            "  training_iteration: 5\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.3/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 706 s, 5 iter, 75005 ts, -2.2e+05 rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:31:27,525\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 21993.0x the scale of `vf_clip_param`. This means that it will take more than 21993.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 3903.8537590113287\n",
            "    bytes_per_rule_mean: 2775.361139718503\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: .nan\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: .nan\n",
            "    exceeded_max_depth_max: 17088\n",
            "    exceeded_max_depth_mean: 10227.166666666666\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 501.0\n",
            "    memory_access_min: 501\n",
            "    memory_access_valid_max: .nan\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: .nan\n",
            "    nodes_remaining_max: 17586\n",
            "    nodes_remaining_mean: 10628.5\n",
            "    nodes_remaining_min: 90\n",
            "    num_nodes_max: 160083\n",
            "    num_nodes_mean: 99837.66666666667\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: .nan\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: .nan\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 15001.0\n",
            "    num_splits_min: 15001\n",
            "    num_splits_valid_max: .nan\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: .nan\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 948.1666666666666\n",
            "    rules_remaining_min: 834\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 501.0\n",
            "    tree_depth_min: 501\n",
            "    tree_depth_valid_max: .nan\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: .nan\n",
            "    update_memory_access_max: 65268\n",
            "    update_memory_access_mean: 49005.166666666664\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_04-35-27\n",
            "  done: false\n",
            "  episode_len_mean: 15001.0\n",
            "  episode_reward_max: -161261.0\n",
            "  episode_reward_mean: -214027.16666666666\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 6\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 35040.433\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 0.44999998807907104\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 2.842489719390869\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.02745395340025425\n",
            "        policy_loss: -0.03773884102702141\n",
            "        total_loss: 2591.24853515625\n",
            "        vf_explained_var: 0.029792821034789085\n",
            "        vf_loss: 2591.302001953125\n",
            "    load_time_ms: 41.605\n",
            "    num_steps_sampled: 90006\n",
            "    num_steps_trained: 90000\n",
            "    sample_time_ms: 122527.071\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 6\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 5.141271885904787\n",
            "    mean_inference_ms: 1.056142882965011\n",
            "    mean_processing_ms: 1.4130058616128158\n",
            "  time_since_restore: 945.8702919483185\n",
            "  time_this_iter_s: 239.7405126094818\n",
            "  time_total_s: 945.8702919483185\n",
            "  timestamp: 1692333327\n",
            "  timesteps_since_restore: 90006\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 90006\n",
            "  training_iteration: 6\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.3/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 945 s, 6 iter, 90006 ts, -2.14e+05 rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:35:27,279\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 21403.0x the scale of `vf_clip_param`. This means that it will take more than 21403.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 3903.8537590113287\n",
            "    bytes_per_rule_mean: 2879.9093717816686\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: .nan\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: .nan\n",
            "    exceeded_max_depth_max: 17088\n",
            "    exceeded_max_depth_mean: 10662.714285714286\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 501.0\n",
            "    memory_access_min: 501\n",
            "    memory_access_valid_max: .nan\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: .nan\n",
            "    nodes_remaining_max: 17586\n",
            "    nodes_remaining_mean: 11085.714285714286\n",
            "    nodes_remaining_min: 90\n",
            "    num_nodes_max: 160083\n",
            "    num_nodes_mean: 98613.0\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: .nan\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: .nan\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 15001.0\n",
            "    num_splits_min: 15001\n",
            "    num_splits_valid_max: .nan\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: .nan\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 941.1428571428571\n",
            "    rules_remaining_min: 834\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 501.0\n",
            "    tree_depth_min: 501\n",
            "    tree_depth_valid_max: .nan\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: .nan\n",
            "    update_memory_access_max: 65268\n",
            "    update_memory_access_mean: 51141.57142857143\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_04-38-11\n",
            "  done: false\n",
            "  episode_len_mean: 15001.0\n",
            "  episode_reward_max: -161261.0\n",
            "  episode_reward_mean: -207057.57142857142\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 7\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 34760.971\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 3.0347681045532227\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.01529918797314167\n",
            "        policy_loss: -0.030171506106853485\n",
            "        total_loss: 2625.96435546875\n",
            "        vf_explained_var: 0.018957102671265602\n",
            "        vf_loss: 2626.0146484375\n",
            "    load_time_ms: 38.466\n",
            "    num_steps_sampled: 105007\n",
            "    num_steps_trained: 105000\n",
            "    sample_time_ms: 123734.954\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 7\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 5.2840619501232435\n",
            "    mean_inference_ms: 1.0568088369363855\n",
            "    mean_processing_ms: 1.359933740128748\n",
            "  time_since_restore: 1109.9956035614014\n",
            "  time_this_iter_s: 164.12531161308289\n",
            "  time_total_s: 1109.9956035614014\n",
            "  timestamp: 1692333491\n",
            "  timesteps_since_restore: 105007\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 105007\n",
            "  training_iteration: 7\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.3/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 1109 s, 7 iter, 105007 ts, -2.07e+05 rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:38:11,417\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 20706.0x the scale of `vf_clip_param`. This means that it will take more than 20706.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 3903.8537590113287\n",
            "    bytes_per_rule_mean: 2852.873583934089\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: .nan\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: .nan\n",
            "    exceeded_max_depth_max: 17088\n",
            "    exceeded_max_depth_mean: 11238.25\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 501.0\n",
            "    memory_access_min: 501\n",
            "    memory_access_valid_max: .nan\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: .nan\n",
            "    nodes_remaining_max: 17586\n",
            "    nodes_remaining_mean: 11666.625\n",
            "    nodes_remaining_min: 90\n",
            "    num_nodes_max: 160083\n",
            "    num_nodes_mean: 97237.25\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: .nan\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: .nan\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 15001.0\n",
            "    num_splits_min: 15001\n",
            "    num_splits_valid_max: .nan\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: .nan\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 944.75\n",
            "    rules_remaining_min: 834\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 501.0\n",
            "    tree_depth_min: 501\n",
            "    tree_depth_valid_max: .nan\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: .nan\n",
            "    update_memory_access_max: 65268\n",
            "    update_memory_access_mean: 52786.375\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_04-40-32\n",
            "  done: false\n",
            "  episode_len_mean: 15001.0\n",
            "  episode_reward_max: -161261.0\n",
            "  episode_reward_mean: -201968.125\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 8\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 35231.971\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 2.9787392616271973\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.012109280563890934\n",
            "        policy_loss: -0.033926088362932205\n",
            "        total_loss: 2580.817626953125\n",
            "        vf_explained_var: 0.04109670966863632\n",
            "        vf_loss: 2580.873291015625\n",
            "    load_time_ms: 35.84\n",
            "    num_steps_sampled: 120008\n",
            "    num_steps_trained: 120000\n",
            "    sample_time_ms: 121075.009\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 8\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 5.373974088740729\n",
            "    mean_inference_ms: 1.0558829986239546\n",
            "    mean_processing_ms: 1.3165846288247376\n",
            "  time_since_restore: 1251.0446298122406\n",
            "  time_this_iter_s: 141.04902625083923\n",
            "  time_total_s: 1251.0446298122406\n",
            "  timestamp: 1692333632\n",
            "  timesteps_since_restore: 120008\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 120008\n",
            "  training_iteration: 8\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.4/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 1251 s, 8 iter, 120008 ts, -2.02e+05 rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:40:32,478\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 20197.0x the scale of `vf_clip_param`. This means that it will take more than 20197.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 3903.8537590113287\n",
            "    bytes_per_rule_mean: 2860.0894839226457\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: .nan\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: .nan\n",
            "    exceeded_max_depth_max: 17088\n",
            "    exceeded_max_depth_mean: 11304.444444444445\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 501.0\n",
            "    memory_access_min: 501\n",
            "    memory_access_valid_max: .nan\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: .nan\n",
            "    nodes_remaining_max: 17586\n",
            "    nodes_remaining_mean: 11732.555555555555\n",
            "    nodes_remaining_min: 90\n",
            "    num_nodes_max: 160083\n",
            "    num_nodes_mean: 97108.11111111111\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: .nan\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: .nan\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 15001.0\n",
            "    num_splits_min: 15001\n",
            "    num_splits_valid_max: .nan\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: .nan\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 934.0\n",
            "    rules_remaining_min: 834\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 501.0\n",
            "    tree_depth_min: 501\n",
            "    tree_depth_valid_max: .nan\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: .nan\n",
            "    update_memory_access_max: 65268\n",
            "    update_memory_access_mean: 54068.444444444445\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_04-42-49\n",
            "  done: false\n",
            "  episode_len_mean: 15001.0\n",
            "  episode_reward_max: -161261.0\n",
            "  episode_reward_mean: -198097.22222222222\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 9\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 35234.241\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 3.0107736587524414\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.018506277352571487\n",
            "        policy_loss: -0.031830932945013046\n",
            "        total_loss: 2918.90966796875\n",
            "        vf_explained_var: 0.00462321424856782\n",
            "        vf_loss: 2918.95947265625\n",
            "    load_time_ms: 33.588\n",
            "    num_steps_sampled: 135009\n",
            "    num_steps_trained: 135000\n",
            "    sample_time_ms: 118910.117\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 9\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 5.432444999986349\n",
            "    mean_inference_ms: 1.0538961619499128\n",
            "    mean_processing_ms: 1.2795692173655167\n",
            "  time_since_restore: 1387.9560241699219\n",
            "  time_this_iter_s: 136.91139435768127\n",
            "  time_total_s: 1387.9560241699219\n",
            "  timestamp: 1692333769\n",
            "  timesteps_since_restore: 135009\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 135009\n",
            "  training_iteration: 9\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:42:49,404\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 19810.0x the scale of `vf_clip_param`. This means that it will take more than 19810.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.4/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 1387 s, 9 iter, 135009 ts, -1.98e+05 rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:44:25,946\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 18205.0x the scale of `vf_clip_param`. This means that it will take more than 18205.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 4419.132852729145\n",
            "    bytes_per_rule_mean: 3015.9938208032954\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: .nan\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: .nan\n",
            "    exceeded_max_depth_max: 17088\n",
            "    exceeded_max_depth_mean: 10174.0\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 459.7\n",
            "    memory_access_min: 88\n",
            "    memory_access_valid_max: .nan\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: .nan\n",
            "    nodes_remaining_max: 17586\n",
            "    nodes_remaining_mean: 10573.0\n",
            "    nodes_remaining_min: 90\n",
            "    num_nodes_max: 160083\n",
            "    num_nodes_mean: 101240.4\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: .nan\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: .nan\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 15001.0\n",
            "    num_splits_min: 15001\n",
            "    num_splits_valid_max: .nan\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: .nan\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 937.5\n",
            "    rules_remaining_min: 834\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 459.7\n",
            "    tree_depth_min: 88\n",
            "    tree_depth_valid_max: .nan\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: .nan\n",
            "    update_memory_access_max: 112608\n",
            "    update_memory_access_mean: 59922.4\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_04-44-25\n",
            "  done: false\n",
            "  episode_len_mean: 15001.0\n",
            "  episode_reward_max: -37629.0\n",
            "  episode_reward_mean: -182050.4\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 10\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 35611.053\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 0.675000011920929\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 2.421046495437622\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.023591959848999977\n",
            "        policy_loss: -0.06636986881494522\n",
            "        total_loss: 24.07308006286621\n",
            "        vf_explained_var: 0.017699750140309334\n",
            "        vf_loss: 24.147733688354492\n",
            "    load_time_ms: 32.257\n",
            "    num_steps_sampled: 150010\n",
            "    num_steps_trained: 150000\n",
            "    sample_time_ms: 112763.193\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 10\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 5.439407741839539\n",
            "    mean_inference_ms: 1.051671404489408\n",
            "    mean_processing_ms: 1.2494290788877553\n",
            "  time_since_restore: 1484.4831564426422\n",
            "  time_this_iter_s: 96.52713227272034\n",
            "  time_total_s: 1484.4831564426422\n",
            "  timestamp: 1692333865\n",
            "  timesteps_since_restore: 150010\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 150010\n",
            "  training_iteration: 10\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.3/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 1484 s, 10 iter, 150010 ts, -1.82e+05 rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m Saving tree to /content/drive/MyDrive/CIAL/neurocut_update/w=1,u=0/fw2_1k/fw2_1k-22-acc-45366-uacc-4342.47-bytes-1692333968.6811721.pkl\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 4972.329557157569\n",
            "    bytes_per_rule_mean: 3289.5612770339862\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: 4972.329557157569\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: 4342.467559217302\n",
            "    exceeded_max_depth_max: 17088\n",
            "    exceeded_max_depth_mean: 8478.333333333334\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 387.4166666666667\n",
            "    memory_access_min: 22\n",
            "    memory_access_valid_max: 30\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: 22\n",
            "    nodes_remaining_max: 17586\n",
            "    nodes_remaining_mean: 8810.833333333334\n",
            "    nodes_remaining_min: 0\n",
            "    num_nodes_max: 160083\n",
            "    num_nodes_mean: 103732.91666666667\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: 126972\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: 105419\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 14019.916666666666\n",
            "    num_splits_min: 8340\n",
            "    num_splits_valid_max: 9889\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: 8340\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 781.25\n",
            "    rules_remaining_min: 0\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 387.4166666666667\n",
            "    tree_depth_min: 22\n",
            "    tree_depth_valid_max: 30\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: 22\n",
            "    update_memory_access_max: 112608\n",
            "    update_memory_access_mean: 58146.75\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_04-49-39\n",
            "  done: false\n",
            "  episode_len_mean: 14019.916666666666\n",
            "  episode_reward_max: -23624.0\n",
            "  episode_reward_mean: -156071.33333333334\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 2\n",
            "  episodes_total: 12\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 36237.328\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 1.0125000476837158\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 2.530003309249878\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.034110162407159805\n",
            "        policy_loss: -0.10834566503763199\n",
            "        total_loss: 6.451170444488525\n",
            "        vf_explained_var: 0.14799731969833374\n",
            "        vf_loss: 6.5502800941467285\n",
            "    load_time_ms: 19.044\n",
            "    num_steps_sampled: 168239\n",
            "    num_steps_trained: 168000\n",
            "    sample_time_ms: 129562.811\n",
            "    update_time_ms: 0.002\n",
            "  iterations_since_restore: 11\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 5.58140968005048\n",
            "    mean_inference_ms: 1.051932327319205\n",
            "    mean_processing_ms: 1.2025385373319555\n",
            "  time_since_restore: 1797.5996551513672\n",
            "  time_this_iter_s: 313.116498708725\n",
            "  time_total_s: 1797.5996551513672\n",
            "  timestamp: 1692334179\n",
            "  timesteps_since_restore: 168239\n",
            "  timesteps_this_iter: 18229\n",
            "  timesteps_total: 168239\n",
            "  training_iteration: 11\n",
            "  trial_id: 6d9a7298\n",
            "  \u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 04:49:39,091\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 15607.0x the scale of `vf_clip_param`. This means that it will take more than 15607.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.3/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 1797 s, 11 iter, 168239 ts, -1.56e+05 rew\n",
            "\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 6092.739443872297\n",
            "    bytes_per_rule_mean: 3505.1903667907786\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: 4972.329557157569\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: 4342.467559217302\n",
            "    exceeded_max_depth_max: 17088\n",
            "    exceeded_max_depth_mean: 8216.615384615385\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 396.15384615384613\n",
            "    memory_access_min: 22\n",
            "    memory_access_valid_max: 30\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: 22\n",
            "    nodes_remaining_max: 17586\n",
            "    nodes_remaining_mean: 8563.0\n",
            "    nodes_remaining_min: 0\n",
            "    num_nodes_max: 161187\n",
            "    num_nodes_mean: 108152.46153846153\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: 126972\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: 105419\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 14095.384615384615\n",
            "    num_splits_min: 8340\n",
            "    num_splits_valid_max: 9889\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: 8340\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 791.1538461538462\n",
            "    rules_remaining_min: 0\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 396.15384615384613\n",
            "    tree_depth_min: 22\n",
            "    tree_depth_valid_max: 30\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: 22\n",
            "    update_memory_access_max: 112608\n",
            "    update_memory_access_mean: 59590.230769230766\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_05-00-29\n",
            "  done: false\n",
            "  episode_len_mean: 14095.384615384615\n",
            "  episode_reward_max: -23624.0\n",
            "  episode_reward_mean: -157266.46153846153\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 13\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 35939.371\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 1.5187499523162842\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 2.76489520072937\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.006727577652782202\n",
            "        policy_loss: -0.02455240860581398\n",
            "        total_loss: 2724.46875\n",
            "        vf_explained_var: -0.000357445067493245\n",
            "        vf_loss: 2724.5107421875\n",
            "    load_time_ms: 19.518\n",
            "    num_steps_sampled: 183240\n",
            "    num_steps_trained: 183000\n",
            "    sample_time_ms: 176051.992\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 12\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 5.841332653467374\n",
            "    mean_inference_ms: 1.0530386179329227\n",
            "    mean_processing_ms: 1.1848980200101014\n",
            "  time_since_restore: 2448.3658969402313\n",
            "  time_this_iter_s: 650.7662417888641\n",
            "  time_total_s: 2448.3658969402313\n",
            "  timestamp: 1692334829\n",
            "  timesteps_since_restore: 183240\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 183240\n",
            "  training_iteration: 12\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 05:00:29,881\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 15727.0x the scale of `vf_clip_param`. This means that it will take more than 15727.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.4/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 2448 s, 12 iter, 183240 ts, -1.57e+05 rew\n",
            "\n",
            "Result for PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\n",
            "  custom_metrics:\n",
            "    bytes_per_rule_max: 6448.871266735325\n",
            "    bytes_per_rule_mean: 3715.4532882153894\n",
            "    bytes_per_rule_min: 1576.961894953656\n",
            "    bytes_per_rule_valid_max: 4972.329557157569\n",
            "    bytes_per_rule_valid_mean: .nan\n",
            "    bytes_per_rule_valid_min: 4342.467559217302\n",
            "    exceeded_max_depth_max: 17088\n",
            "    exceeded_max_depth_mean: 8181.857142857143\n",
            "    exceeded_max_depth_min: 0\n",
            "    memory_access_max: 501\n",
            "    memory_access_mean: 403.64285714285717\n",
            "    memory_access_min: 22\n",
            "    memory_access_valid_max: 30\n",
            "    memory_access_valid_mean: .nan\n",
            "    memory_access_valid_min: 22\n",
            "    nodes_remaining_max: 17586\n",
            "    nodes_remaining_mean: 8536.785714285714\n",
            "    nodes_remaining_min: 0\n",
            "    num_nodes_max: 161187\n",
            "    num_nodes_mean: 107902.64285714286\n",
            "    num_nodes_min: 61555\n",
            "    num_nodes_valid_max: 126972\n",
            "    num_nodes_valid_mean: .nan\n",
            "    num_nodes_valid_min: 105419\n",
            "    num_splits_max: 15001\n",
            "    num_splits_mean: 14160.07142857143\n",
            "    num_splits_min: 8340\n",
            "    num_splits_valid_max: 9889\n",
            "    num_splits_valid_mean: .nan\n",
            "    num_splits_valid_min: 8340\n",
            "    partition_fraction_max: 0.0\n",
            "    partition_fraction_mean: 0.0\n",
            "    partition_fraction_min: 0.0\n",
            "    rules_remaining_max: 971\n",
            "    rules_remaining_mean: 803.6428571428571\n",
            "    rules_remaining_min: 0\n",
            "    tree_depth_max: 501\n",
            "    tree_depth_mean: 403.64285714285717\n",
            "    tree_depth_min: 22\n",
            "    tree_depth_valid_max: 30\n",
            "    tree_depth_valid_mean: .nan\n",
            "    tree_depth_valid_min: 22\n",
            "    update_memory_access_max: 112608\n",
            "    update_memory_access_mean: 61101.92857142857\n",
            "    update_memory_access_min: 22426\n",
            "  date: 2023-08-18_05-03-50\n",
            "  done: false\n",
            "  episode_len_mean: 14160.07142857143\n",
            "  episode_reward_max: -23624.0\n",
            "  episode_reward_mean: -158570.14285714287\n",
            "  episode_reward_min: -440932.0\n",
            "  episodes_this_iter: 1\n",
            "  episodes_total: 14\n",
            "  experiment_id: 59cae4d49b9748109625bc0627fd10d5\n",
            "  hostname: e6832a943b6c\n",
            "  info:\n",
            "    grad_time_ms: 36331.024\n",
            "    learner:\n",
            "      default_policy:\n",
            "        cur_kl_coeff: 1.5187499523162842\n",
            "        cur_lr: 4.999999873689376e-05\n",
            "        entropy: 3.04072904586792\n",
            "        entropy_coeff: 0.009999999776482582\n",
            "        kl: 0.005629098974168301\n",
            "        policy_loss: -0.03004656918346882\n",
            "        total_loss: 2659.76123046875\n",
            "        vf_explained_var: 0.008970323950052261\n",
            "        vf_loss: 2659.813232421875\n",
            "    load_time_ms: 19.998\n",
            "    num_steps_sampled: 198241\n",
            "    num_steps_trained: 198000\n",
            "    sample_time_ms: 183631.245\n",
            "    update_time_ms: 0.003\n",
            "  iterations_since_restore: 13\n",
            "  node_ip: 172.28.0.12\n",
            "  num_healthy_workers: 0\n",
            "  off_policy_estimator: {}\n",
            "  pid: 9042\n",
            "  policy_reward_max: {}\n",
            "  policy_reward_mean: {}\n",
            "  policy_reward_min: {}\n",
            "  sampler_perf:\n",
            "    mean_env_wait_ms: 6.063555578523737\n",
            "    mean_inference_ms: 1.0537259124804776\n",
            "    mean_processing_ms: 1.1697522609461468\n",
            "  time_since_restore: 2648.6879947185516\n",
            "  time_this_iter_s: 200.3220977783203\n",
            "  time_total_s: 2648.6879947185516\n",
            "  timestamp: 1692335030\n",
            "  timesteps_since_restore: 198241\n",
            "  timesteps_this_iter: 15001\n",
            "  timesteps_total: 198241\n",
            "  training_iteration: 13\n",
            "  trial_id: 6d9a7298\n",
            "  \n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m 2023-08-18 05:03:50,226\tWARNING ppo.py:125 -- The magnitude of your environment rewards are more than 15857.0x the scale of `vf_clip_param`. This means that it will take more than 15857.0 iterations for your value function to converge. If this is not intended, consider increasing `vf_clip_param`.\n",
            "== Status ==\n",
            "Using FIFO scheduling algorithm.\n",
            "Resources requested: 1/2 CPUs, 0/0 GPUs, 0.0/6.84 GiB heap, 0.0/2.34 GiB objects\n",
            "Memory usage on this node: 2.4/12.7 GiB\n",
            "Result logdir: /root/ray_results/neurocuts_None\n",
            "Number of trials: 1 ({'RUNNING': 1})\n",
            "RUNNING trials:\n",
            " - PPO_tree_env_0_rules=_content_NeuroCuts_robin_ruleset_fw2_1k,tree_gae_lambda=0.95:\tRUNNING, [1 CPUs, 0 GPUs], [pid=9042], 2648 s, 13 iter, 198241 ts, -1.59e+05 rew\n",
            "\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m Saving tree to /content/drive/MyDrive/CIAL/neurocut_update/w=1,u=0/fw2_1k/fw2_1k-16-acc-41582-uacc-3862.73-bytes-1692335113.061108.pkl\n",
            "\u001b[2m\u001b[36m(pid=9042)\u001b[0m Saving tree to /content/drive/MyDrive/CIAL/neurocut_update/w=1,u=0/fw2_1k/fw2_1k-19-acc-37017-uacc-3465.6-bytes-1692335161.716771.pkl\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "運行原方法(版本3、版本4) 步驟\n",
        "\n",
        "neurocuts_env.py\n",
        "\n",
        "```\n",
        "# 此內容會顯示為程式碼\n",
        "import collections\n",
        "import time\n",
        "import os\n",
        "import numpy as np\n",
        "import pickle\n",
        "from gym.spaces import Tuple, Box, Discrete, Dict\n",
        "\n",
        "from ray.rllib.env import MultiAgentEnv\n",
        "from ray.rllib.evaluation.rollout_worker import get_global_worker\n",
        "\n",
        "from tree import Tree, load_rules_from_file\n",
        "from hicuts import HiCuts\n",
        "\n",
        "NUM_DIMENSIONS = 5\n",
        "NUM_PART_LEVELS = 6  # 2%, 4%, 8%, 16%, 32%, 64%\n",
        "\n",
        "\n",
        "class NeuroCutsEnv(MultiAgentEnv):\n",
        "    \"\"\"NeuroCuts multi-agent tree building environment.\n",
        "\n",
        "    In this env, each \"cut\" in the tree is an action taken by a\n",
        "    different agent. All the agents share the same policy. We\n",
        "    aggregate rewards at the end of the episode and assign each\n",
        "    cut its reward based on the policy performance (actual depth).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "                 rules_file,\n",
        "                 leaf_threshold=16,\n",
        "                 max_cuts_per_dimension=5,\n",
        "                 max_actions_per_episode=5000,\n",
        "                 max_depth=100,\n",
        "                 partition_mode=None,\n",
        "                 reward_shape=\"linear\",\n",
        "                 depth_weight=1.0,\n",
        "                 update_weight = 0, ##\n",
        "                 dump_dir=None,\n",
        "                 tree_gae=True,\n",
        "                 tree_gae_gamma=1.0,\n",
        "                 tree_gae_lambda=0.95,\n",
        "                 zero_obs=False):\n",
        "\n",
        "        self.tree_gae = tree_gae\n",
        "        self.tree_gae_gamma = tree_gae_gamma\n",
        "        self.tree_gae_lambda = tree_gae_lambda\n",
        "        self.reward_shape = {\n",
        "            \"linear\": lambda x: x,\n",
        "            \"log\": lambda x: np.log(x),\n",
        "        }[reward_shape]\n",
        "        self.zero_obs = zero_obs\n",
        "\n",
        "        assert partition_mode in [None, \"simple\", \"efficuts\", \"cutsplit\"]\n",
        "        self.partition_enabled = partition_mode == \"simple\"\n",
        "        if partition_mode in [\"efficuts\", \"cutsplit\"]:\n",
        "            self.force_partition = partition_mode\n",
        "        else:\n",
        "            self.force_partition = False\n",
        "\n",
        "        self.dump_dir = dump_dir and os.path.expanduser(dump_dir)\n",
        "        if self.dump_dir:\n",
        "            try:\n",
        "                os.makedirs(self.dump_dir)\n",
        "            except:\n",
        "                pass\n",
        "        self.best_time = float(\"inf\")\n",
        "        self.best_space = float(\"inf\")\n",
        "        self.best_utime = float(\"inf\") ##\n",
        "\n",
        "        self.depth_weight = depth_weight\n",
        "        self.update_weight = update_weight ##\n",
        "        self.rules_file = rules_file\n",
        "        self.rules = load_rules_from_file(rules_file)\n",
        "        self.leaf_threshold = leaf_threshold\n",
        "        self.max_actions_per_episode = max_actions_per_episode\n",
        "        self.max_depth = max_depth\n",
        "        self.num_actions = None\n",
        "        self.tree = None\n",
        "        self.node_map = None\n",
        "        self.child_map = None\n",
        "        self.max_cuts_per_dimension = max_cuts_per_dimension\n",
        "        if self.partition_enabled:\n",
        "            self.num_part_levels = NUM_PART_LEVELS\n",
        "        else:\n",
        "            self.num_part_levels = 0\n",
        "        self.action_space = Tuple([\n",
        "            Discrete(NUM_DIMENSIONS),\n",
        "            Discrete(max_cuts_per_dimension + self.num_part_levels)\n",
        "        ])\n",
        "        self.observation_space = Dict({\n",
        "            \"real_obs\": Box(0, 99999999, (279, ), dtype=np.float32),\n",
        "            \"action_mask\": Box(\n",
        "                0,\n",
        "                1, (NUM_DIMENSIONS + max_cuts_per_dimension + self.num_part_levels, ),\n",
        "                dtype=np.float32),\n",
        "        })\n",
        "\n",
        "    def reset(self):\n",
        "        self.num_actions = 0\n",
        "        self.exceeded_max_depth = []\n",
        "        self.tree = Tree(\n",
        "            self.rules,\n",
        "            self.leaf_threshold,\n",
        "            refinements={\n",
        "                \"node_merging\": True,\n",
        "                \"rule_overlay\": True,\n",
        "                \"region_compaction\": False,\n",
        "                \"rule_pushup\": False,\n",
        "                \"equi_dense\": False,\n",
        "            })\n",
        "        self.node_map = {\n",
        "            self.tree.root.id: self.tree.root,\n",
        "        }\n",
        "        self.child_map = {}\n",
        "\n",
        "        if self.force_partition:\n",
        "            if self.force_partition == \"cutsplit\":\n",
        "                self.tree.partition_cutsplit()\n",
        "            elif self.force_partition == \"efficuts\":\n",
        "                self.tree.partition_efficuts()\n",
        "            else:\n",
        "                assert False, self.force_partition\n",
        "            for c in self.tree.root.children:\n",
        "                self.node_map[c.id] = c\n",
        "            self.child_map[self.tree.root.id] = [\n",
        "                c.id for c in self.tree.root.children\n",
        "            ]\n",
        "\n",
        "        start = self.tree.current_node\n",
        "        return {start.id: self._encode_state(start)}\n",
        "\n",
        "    def step(self, action_dict):\n",
        "        assert len(action_dict) == 1  # one at a time processing\n",
        "\n",
        "        new_children = []\n",
        "        for node_id, action in action_dict.items():\n",
        "            node = self.node_map[node_id]\n",
        "            orig_action = action\n",
        "            if np.isscalar(action):\n",
        "                assert not self.partition_enabled, action\n",
        "                partition = False\n",
        "                cut_dimension = int(action) % 5\n",
        "                cut_num = int(action) // 5\n",
        "                action = [cut_dimension, cut_num]\n",
        "            else:\n",
        "                if action[1] >= self.max_cuts_per_dimension:\n",
        "                    assert self.partition_enabled, (\n",
        "                        action, self.max_cuts_per_dimension)\n",
        "                    partition = True\n",
        "                    action[1] -= self.max_cuts_per_dimension\n",
        "                else:\n",
        "                    partition = False\n",
        "\n",
        "            if partition:\n",
        "                children = self.tree.partition_node(node, action[0], action[1])\n",
        "            else:\n",
        "                cut_dimension, cut_num = self.action_tuple_to_cut(node, action)\n",
        "                children = self.tree.cut_node(node, cut_dimension,\n",
        "                                              int(cut_num))\n",
        "\n",
        "            self.num_actions += 1\n",
        "            num_leaf = 0\n",
        "            for c in children:\n",
        "                self.node_map[c.id] = c\n",
        "                if not self.tree.is_leaf(c):\n",
        "                    new_children.append(c)\n",
        "                else:\n",
        "                    num_leaf += 1\n",
        "            self.child_map[node_id] = [c.id for c in children]\n",
        "\n",
        "        node = self.tree.get_current_node()\n",
        "        while node and (self.tree.is_leaf(node)\n",
        "                        or node.depth > self.max_depth):\n",
        "            node = self.tree.get_next_node()\n",
        "            if node and node.depth > self.max_depth:\n",
        "                self.exceeded_max_depth.append(node)\n",
        "        nodes_remaining = self.tree.nodes_to_cut + self.exceeded_max_depth\n",
        "\n",
        "        obs, rew, done, info = {}, {}, {}, {}\n",
        "\n",
        "        if (not nodes_remaining\n",
        "                or self.num_actions > self.max_actions_per_episode\n",
        "                or self.tree.get_current_node() is None):\n",
        "            zero_state = self._zeros()\n",
        "            rew = self.compute_rewards(self.depth_weight,self.update_weight) ##\n",
        "            stats = {}\n",
        "            obs = {node_id: zero_state for node_id in rew.keys()}\n",
        "            if self.tree_gae:\n",
        "                advantages, stats = self.compute_gae(self.depth_weight)\n",
        "                info = {\n",
        "                    node_id: {\n",
        "                        \"__advantage__\": advantages[node_id],\n",
        "                        \"__value_target__\": rew[node_id],\n",
        "                    }\n",
        "                    for node_id in rew.keys()\n",
        "                }\n",
        "            else:\n",
        "                info = {node_id: {} for node_id in rew.keys()}\n",
        "            result = self.tree.compute_result()\n",
        "            rules_remaining = set()\n",
        "            for n in nodes_remaining:\n",
        "                for r in n.rules:\n",
        "                    rules_remaining.add(str(r))\n",
        "            info[self.tree.root.id].update({\n",
        "                \"bytes_per_rule\": result[\"bytes_per_rule\"],\n",
        "                \"memory_access\": result[\"memory_access\"],\n",
        "                \"update_memory_access\": result[\"update_memory_access\"],\n",
        "                \"exceeded_max_depth\": len(self.exceeded_max_depth),\n",
        "                \"tree_depth\": self.tree.get_depth(),\n",
        "                \"tree_stats\": self.tree.get_stats(),\n",
        "                \"tree_stats_str\": self.tree.stats_str(),\n",
        "                \"nodes_remaining\": len(nodes_remaining),\n",
        "                \"rules_remaining\": len(rules_remaining),\n",
        "                \"num_nodes\": len(self.node_map),\n",
        "                \"partition_fraction\": float(\n",
        "                    len([\n",
        "                        n for n in self.node_map.values() if n.is_partition()\n",
        "                    ])) / len(self.node_map),\n",
        "                \"num_splits\": self.num_actions,\n",
        "                \"rules_file\": self.rules_file,\n",
        "            })\n",
        "            info[self.tree.root.id].update(stats)\n",
        "            if not nodes_remaining and self.dump_dir:\n",
        "                self.save_if_best(result)\n",
        "            return obs, rew, {\"__all__\": True}, info\n",
        "\n",
        "        needs_split = [self.tree.get_current_node()]\n",
        "        obs.update({s.id: self._encode_state(s) for s in needs_split})\n",
        "        rew.update({s.id: 0 for s in needs_split})\n",
        "        done.update({\"__all__\": False})\n",
        "        info.update({s.id: {} for s in needs_split})\n",
        "        return obs, rew, done, info\n",
        "\n",
        "    def save_if_best(self, result):\n",
        "        time_stat = int(result[\"memory_access\"])\n",
        "        space_stat = round(float(result[\"bytes_per_rule\"]),2)\n",
        "        update_stat = int(result[\"update_memory_access\"])\n",
        "        save = False\n",
        "        if time_stat < self.best_time:\n",
        "            self.best_time = time_stat\n",
        "            save = True\n",
        "        if space_stat < self.best_space:\n",
        "            self.best_space = space_stat\n",
        "            save = True\n",
        "        if update_stat < self.best_utime:\n",
        "            self.best_utime = update_stat\n",
        "            save = True\n",
        "        if save:\n",
        "            out = os.path.join(\n",
        "                self.dump_dir, \"{}-{}-acc-{}-uacc-{}-bytes-{}.pkl\".format(\n",
        "                    os.path.basename(self.rules_file), time_stat, update_stat, space_stat,\n",
        "                    time.time()))\n",
        "            print(\"Saving tree to {}\".format(out))\n",
        "            with open(out, \"wb\") as f:\n",
        "                pickle.dump(self.tree, f)\n",
        "\n",
        "    def action_tuple_to_cut(self, node, action):\n",
        "        cut_dimension = action[0]\n",
        "        range_left = node.ranges[cut_dimension * 2]\n",
        "        range_right = node.ranges[cut_dimension * 2 + 1]\n",
        "        cut_num = max(2, min(2**(action[1] + 1), range_right - range_left))\n",
        "        return (cut_dimension, cut_num)\n",
        "\n",
        "    def compute_gae(self, depth_weight):\n",
        "        \"\"\"Compute GAE for a branching decision environment.\n",
        "\n",
        "           V(d) = min over nodes n at depth=d V(n)\n",
        "        \"\"\"\n",
        "\n",
        "        assert depth_weight == 1.0, \"GAE not supported with space weight\"\n",
        "\n",
        "        # First precompute the value of each node\n",
        "        V = {}\n",
        "        stats = {}\n",
        "        ev = get_global_worker()\n",
        "        assert ev.policy_config[\"use_gae\"], ev.policy_config[\"use_gae\"]\n",
        "        assert ev.policy_config[\"lambda\"] == 1.0, ev.policy_config[\"lambda\"]\n",
        "        policy = ev.get_policy()\n",
        "        prep = ev.preprocessors[\"default_policy\"]\n",
        "        nlist = list(self.node_map.items())\n",
        "        feed_dict = {\n",
        "            policy.get_placeholder(\"obs\"): [\n",
        "                prep.transform(self._encode_state(node)) for (_, node) in nlist\n",
        "            ],\n",
        "            policy.get_placeholder(\"prev_actions\"): [[0, 0] for _ in nlist],\n",
        "            policy.get_placeholder(\"prev_rewards\"): [0.0 for _ in nlist],\n",
        "            policy.model.seq_lens: [1 for _ in nlist],\n",
        "        }\n",
        "        vf = policy.sess.run(policy.value_function, feed_dict)\n",
        "        V_root = 0.0\n",
        "        for (node_id, _), v in zip(nlist, list(vf)):\n",
        "            V[node_id] = v\n",
        "            if node_id == self.tree.root.id:\n",
        "                V_root = v\n",
        "\n",
        "#        print(\n",
        "#            \"Computed node values\",\n",
        "#            \"mean\", np.mean(vf), \"min\", np.min(vf), \"max\", np.max(vf),\n",
        "#            \"count\", len(vf))\n",
        "        stats[\"V_gae_min\"] = float(np.min(vf))\n",
        "        stats[\"V_gae_max\"] = float(np.max(vf))\n",
        "        stats[\"V_gae_mean\"] = float(np.mean(vf))\n",
        "        stats[\"V_gae_root\"] = float(V_root)\n",
        "\n",
        "        gamma = self.tree_gae_gamma\n",
        "        lambd = self.tree_gae_lambda\n",
        "\n",
        "        # Map from node_id -> depth -> min(V at depth)\n",
        "        # These values are unique per (node_id, depth) combination\n",
        "        min_V_for_node = collections.defaultdict(dict)\n",
        "\n",
        "        # Then, compute the min V at each level for each subtree\n",
        "        incomplete = True\n",
        "        while incomplete:\n",
        "            incomplete = False\n",
        "            for node_id, node in self.node_map.items():\n",
        "                if node_id in min_V_for_node:\n",
        "                    continue\n",
        "\n",
        "                children = self.child_map.get(node_id, [])\n",
        "                if self.tree.is_leaf(node):\n",
        "                    min_V_for_node[node_id][node.depth] = -1\n",
        "                elif not children:\n",
        "                    min_V_for_node[node_id][node.depth] = V[node_id]\n",
        "                elif all((c_id in min_V_for_node) for c_id in children):\n",
        "                    min_V = {}\n",
        "                    for c_id in children:\n",
        "                        for depth, minv in min_V_for_node[c_id].items():\n",
        "                            if depth not in min_V:\n",
        "                                min_V[depth] = minv\n",
        "                            else:\n",
        "                                min_V[depth] = min(min_V[depth], minv)\n",
        "                    assert node.depth not in min_V, min_V\n",
        "                    min_V[node.depth] = V[node_id]\n",
        "                    min_V_for_node[node_id] = min_V\n",
        "                else:\n",
        "                    incomplete = True\n",
        "                    continue\n",
        "#                print(\n",
        "#                    \"Computed minV for node\", node_id, \"depth\", node.depth,\n",
        "#                    min_V_for_node[node_id])\n",
        "\n",
        "# delta(V)_{t+1} in the GAE paper\n",
        "\n",
        "        def deltaV(node_id, depth):\n",
        "            dv = -1 + gamma * min_V_for_node[node_id].get(d + 1, 0.0)\n",
        "            dv -= min_V_for_node[node_id][d]\n",
        "            return dv\n",
        "\n",
        "        # Now we can compute GAE estimates for each\n",
        "        advantages = {}\n",
        "        adv_list = []\n",
        "        adv_root = 0.0\n",
        "        for node_id, node in self.node_map.items():\n",
        "            A_gae = 0.0\n",
        "            d = node.depth\n",
        "            while d in min_V_for_node[node_id]:\n",
        "                A_gae += (gamma * lambd)**(d - node.depth) * deltaV(node_id, d)\n",
        "                d += 1\n",
        "#            print(\"A_gae for node\", node_id, \"depth\", node.depth, A_gae)\n",
        "            adv_list.append(A_gae)\n",
        "            if node_id == self.tree.root.id:\n",
        "                adv_root = A_gae\n",
        "            advantages[node_id] = A_gae\n",
        "\n",
        "\n",
        "#        print(\n",
        "#            \"GAE advantages\",\n",
        "#            \"min\", np.min(adv_list), \"max\", np.max(adv_list),\n",
        "#            \"mean\", np.mean(adv_list))\n",
        "        stats[\"A_gae_min\"] = float(np.min(adv_list))\n",
        "        stats[\"A_gae_max\"] = float(np.max(adv_list))\n",
        "        stats[\"A_gae_mean\"] = float(np.mean(adv_list))\n",
        "        stats[\"A_gae_root\"] = float(adv_root)\n",
        "\n",
        "        return advantages, stats\n",
        "\n",
        "    def compute_rewards(self, depth_weight, update_weight):\n",
        "        depth_to_go = collections.defaultdict(int)\n",
        "        nodes_to_go = collections.defaultdict(int)\n",
        "        updates_to_go = collections.defaultdict(int) ##\n",
        "        num_updates = 1\n",
        "        while num_updates > 0:\n",
        "            num_updates = 0\n",
        "            for node_id, node in self.node_map.items():\n",
        "                if node_id not in depth_to_go:\n",
        "                    if self.tree.is_leaf(node):\n",
        "                        depth_to_go[node_id] = 0\n",
        "                        nodes_to_go[node_id] = 0\n",
        "                        updates_to_go[node_id] = 0 ##\n",
        "                    else:\n",
        "                        depth_to_go[node_id] = 1\n",
        "                        nodes_to_go[node_id] = 1\n",
        "                        updates_to_go[node_id] = 1 ##\n",
        "                if node_id in self.child_map:\n",
        "                    if self.node_map[node_id].is_partition():\n",
        "                        max_child_depth = self.tree_gae_gamma * sum(\n",
        "                            [depth_to_go[c] for c in self.child_map[node_id]])\n",
        "                        max_child_update = 1+ self.tree_gae_gamma * max(\n",
        "                            [updates_to_go[c] for c in self.child_map[node_id]]) ##\n",
        "                    else:\n",
        "                        max_child_depth = 1 + self.tree_gae_gamma * max(\n",
        "                            [depth_to_go[c] for c in self.child_map[node_id]])\n",
        "                        max_child_update = self.tree_gae_gamma * sum(\n",
        "                            [updates_to_go[c] for c in self.child_map[node_id]]) ##\n",
        "                    if max_child_depth > depth_to_go[node_id]: ##\n",
        "                        depth_to_go[node_id] = max_child_depth ##\n",
        "                        num_updates += 1 ##\n",
        "                    if max_child_update > updates_to_go[node_id]:\n",
        "                        updates_to_go[node_id] = max_child_update\n",
        "                        num_updates += 1\n",
        "                    sum_child_cuts = len(self.child_map[node_id]) + sum(\n",
        "                        [nodes_to_go[c] for c in self.child_map[node_id]])\n",
        "                    if sum_child_cuts > nodes_to_go[node_id]:\n",
        "                        nodes_to_go[node_id] = sum_child_cuts\n",
        "                        num_updates += 1\n",
        "        rew = {\n",
        "            node_id:\n",
        "            -depth_weight * self.reward_shape(depth) - update_weight * self.reward_shape(float(updates_to_go[node_id])) -  (1.0 - depth_weight- update_weight) *\n",
        "            self.reward_shape(float(nodes_to_go[node_id]))\n",
        "            for (node_id, depth) in depth_to_go.items()\n",
        "            if node_id in self.child_map\n",
        "        }\n",
        "        return rew\n",
        "\n",
        "    def _zeros(self):\n",
        "        zeros = np.array([0] * 279)\n",
        "        return {\n",
        "            \"real_obs\": zeros,\n",
        "            \"action_mask\": np.array([1] *\n",
        "                (5 + self.max_cuts_per_dimension + self.num_part_levels)),\n",
        "        }\n",
        "\n",
        "    def _encode_state(self, node):\n",
        "        if node.depth > 1:\n",
        "            action_mask = ([1] * (NUM_DIMENSIONS + self.max_cuts_per_dimension) +\n",
        "                           [0] * self.num_part_levels)\n",
        "        else:\n",
        "            assert node.depth == 1, node.depth\n",
        "            action_mask = ([1] * (NUM_DIMENSIONS + self.max_cuts_per_dimension)\n",
        "                           + [1] * self.num_part_levels)\n",
        "        s = np.array(node.get_state())\n",
        "        return {\n",
        "            \"real_obs\": np.zeros_like(s) if self.zero_obs else s,\n",
        "            \"action_mask\": np.array(action_mask),\n",
        "        }\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "tree.py\n",
        "\n",
        "```\n",
        "# 此內容會顯示為程式碼\n",
        "import math\n",
        "import random\n",
        "import numpy as np\n",
        "import re\n",
        "import sys\n",
        "\n",
        "sys.setrecursionlimit(99999)\n",
        "SPLIT_CACHE = {}\n",
        "\n",
        "\n",
        "class Rule:\n",
        "    def __init__(self, priority, ranges):\n",
        "        # each range is left inclusive and right exclusive, i.e., [left, right)\n",
        "        self.priority = priority\n",
        "        self.ranges = ranges\n",
        "        self.names = [\"src_ip\", \"dst_ip\", \"src_port\", \"dst_port\", \"proto\"]\n",
        "\n",
        "    def is_intersect(self, dimension, left, right):\n",
        "        return not (left >= self.ranges[dimension*2+1] or \\\n",
        "            right <= self.ranges[dimension*2])\n",
        "\n",
        "    def is_intersect_multi_dimension(self, ranges):\n",
        "        for i in range(5):\n",
        "            if ranges[i*2] >= self.ranges[i*2+1] or \\\n",
        "                    ranges[i*2+1] <= self.ranges[i*2]:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def sample_packet(self):\n",
        "        src_ip = random.randint(self.ranges[0], self.ranges[1] - 1)\n",
        "        dst_ip = random.randint(self.ranges[2], self.ranges[3] - 1)\n",
        "        src_port = random.randint(self.ranges[4], self.ranges[5] - 1)\n",
        "        dst_port = random.randint(self.ranges[6], self.ranges[7] - 1)\n",
        "        protocol = random.randint(self.ranges[8], self.ranges[9] - 1)\n",
        "        packet = (src_ip, dst_ip, src_port, dst_port, protocol)\n",
        "        assert self.matches(packet), packet\n",
        "        return packet\n",
        "\n",
        "    def matches(self, packet):\n",
        "        assert len(packet) == 5, packet\n",
        "        return self.is_intersect_multi_dimension([\n",
        "            packet[0] + 0,  # src ip\n",
        "            packet[0] + 1,\n",
        "            packet[1] + 0,  # dst ip\n",
        "            packet[1] + 1,\n",
        "            packet[2] + 0,  # src port\n",
        "            packet[2] + 1,\n",
        "            packet[3] + 0,  # dst port\n",
        "            packet[3] + 1,\n",
        "            packet[4] + 0,  # protocol\n",
        "            packet[4] + 1\n",
        "        ])\n",
        "\n",
        "    def is_covered_by(self, other, ranges):\n",
        "        for i in range(5):\n",
        "            if (max(self.ranges[i*2], ranges[i*2]) < \\\n",
        "                    max(other.ranges[i*2], ranges[i*2]))or \\\n",
        "                    (min(self.ranges[i*2+1], ranges[i*2+1]) > \\\n",
        "                    min(other.ranges[i*2+1], ranges[i*2+1])):\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def __str__(self):\n",
        "        result = \"\"\n",
        "        for i in range(len(self.names)):\n",
        "            result += \"%s:[%d, %d) \" % (self.names[i], self.ranges[i * 2],\n",
        "                                        self.ranges[i * 2 + 1])\n",
        "        return result\n",
        "\n",
        "\n",
        "def load_rules_from_file(file_name):\n",
        "    rules = []\n",
        "    rule_fmt = re.compile(r'^@(\\d+).(\\d+).(\\d+).(\\d+)/(\\d+) '\\\n",
        "        r'(\\d+).(\\d+).(\\d+).(\\d+)/(\\d+) ' \\\n",
        "        r'(\\d+) : (\\d+) ' \\\n",
        "        r'(\\d+) : (\\d+) ' \\\n",
        "        r'(0x[\\da-fA-F]+)/(0x[\\da-fA-F]+) ' \\\n",
        "        r'(.*?)')\n",
        "    for idx, line in enumerate(open(file_name)):\n",
        "        elements = line[1:-1].split('\\t')\n",
        "        line = line.replace('\\t', ' ')\n",
        "\n",
        "        sip0, sip1, sip2, sip3, sip_mask_len, \\\n",
        "        dip0, dip1, dip2, dip3, dip_mask_len, \\\n",
        "        sport_begin, sport_end, \\\n",
        "        dport_begin, dport_end, \\\n",
        "        proto, proto_mask = \\\n",
        "        (eval(rule_fmt.match(line).group(i)) for i in range(1, 17))\n",
        "\n",
        "        sip0 = (sip0 << 24) | (sip1 << 16) | (sip2 << 8) | sip3\n",
        "        sip_begin = sip0 & (~((1 << (32 - sip_mask_len)) - 1))\n",
        "        sip_end = sip0 | ((1 << (32 - sip_mask_len)) - 1)\n",
        "\n",
        "        dip0 = (dip0 << 24) | (dip1 << 16) | (dip2 << 8) | dip3\n",
        "        dip_begin = dip0 & (~((1 << (32 - dip_mask_len)) - 1))\n",
        "        dip_end = dip0 | ((1 << (32 - dip_mask_len)) - 1)\n",
        "\n",
        "        if proto_mask == 0xff:\n",
        "            proto_begin = proto\n",
        "            proto_end = proto\n",
        "        else:\n",
        "            proto_begin = 0\n",
        "            proto_end = 0xff\n",
        "\n",
        "        rules.append(\n",
        "            Rule(idx, [\n",
        "                sip_begin, sip_end + 1, dip_begin, dip_end + 1, sport_begin,\n",
        "                sport_end + 1, dport_begin, dport_end + 1, proto_begin,\n",
        "                proto_end + 1\n",
        "            ]))\n",
        "    return rules\n",
        "\n",
        "\n",
        "def to_bits(value, n):\n",
        "    if value >= 2**n:\n",
        "        print(\"WARNING: clamping value\", value, \"to\", 2**n - 1)\n",
        "        value = 2**n - 1\n",
        "    assert value == int(value)\n",
        "    b = list(bin(int(value))[2:])\n",
        "    assert len(b) <= n, (value, b, n)\n",
        "    return [0.0] * (n - len(b)) + [float(i) for i in b]\n",
        "\n",
        "\n",
        "def onehot_encode(arr, n):\n",
        "    out = []\n",
        "    for a in arr:\n",
        "        x = [0] * n\n",
        "        for i in range(a):\n",
        "            x[i] = 1\n",
        "        out.extend(x)\n",
        "    return out\n",
        "\n",
        "\n",
        "class Node:\n",
        "    def __init__(self, id, ranges, rules, depth, partitions, manual_partition):\n",
        "        self.id = id\n",
        "        self.partitions = list(partitions or [])\n",
        "        self.manual_partition = manual_partition\n",
        "        self.ranges = ranges\n",
        "        self.rules = rules\n",
        "        self.depth = depth\n",
        "        self.children = []\n",
        "        self.action = None\n",
        "        self.pushup_rules = None\n",
        "        self.num_rules = len(self.rules)\n",
        "\n",
        "    def is_partition(self):\n",
        "        \"\"\"Returns if node was partitioned.\"\"\"\n",
        "        if not self.action:\n",
        "            return False\n",
        "        elif self.action[0] == \"partition\":\n",
        "            return True\n",
        "        elif self.action[0] == \"cut\":\n",
        "            return False\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def match(self, packet):\n",
        "        if self.is_partition():\n",
        "            matches = []\n",
        "            for c in self.children:\n",
        "                match = c.match(packet)\n",
        "                if match:\n",
        "                    matches.append(match)\n",
        "            if matches:\n",
        "                matches.sort(key=lambda r: self.rules.index(r))\n",
        "                return matches[0]\n",
        "            return None\n",
        "        elif self.children:\n",
        "            for n in self.children:\n",
        "                if n.contains(packet):\n",
        "                    return n.match(packet)\n",
        "            return None\n",
        "        else:\n",
        "            for r in self.rules:\n",
        "                if r.matches(packet):\n",
        "                    return r\n",
        "\n",
        "    def is_intersect_multi_dimension(self, ranges):\n",
        "        for i in range(5):\n",
        "            if ranges[i*2] >= self.ranges[i*2+1] or \\\n",
        "                    ranges[i*2+1] <= self.ranges[i*2]:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def contains(self, packet):\n",
        "        assert len(packet) == 5, packet\n",
        "        return self.is_intersect_multi_dimension([\n",
        "            packet[0] + 0,  # src ip\n",
        "            packet[0] + 1,\n",
        "            packet[1] + 0,  # dst ip\n",
        "            packet[1] + 1,\n",
        "            packet[2] + 0,  # src port\n",
        "            packet[2] + 1,\n",
        "            packet[3] + 0,  # dst port\n",
        "            packet[3] + 1,\n",
        "            packet[4] + 0,  # protocol\n",
        "            packet[4] + 1\n",
        "        ])\n",
        "\n",
        "    def is_useless(self):\n",
        "        if not self.children:\n",
        "            return False\n",
        "        return max(len(c.rules) for c in self.children) == len(self.rules)\n",
        "\n",
        "    def pruned_rules(self):\n",
        "        new_rules = []\n",
        "        for i in range(len(self.rules) - 1):\n",
        "            rule = self.rules[len(self.rules) - 1 - i]\n",
        "            flag = False\n",
        "            for j in range(0, len(self.rules) - 1 - i):\n",
        "                high_priority_rule = self.rules[j]\n",
        "                if rule.is_covered_by(high_priority_rule, self.ranges):\n",
        "                    flag = True\n",
        "                    break\n",
        "            if not flag:\n",
        "                new_rules.append(rule)\n",
        "        new_rules.append(self.rules[0])\n",
        "        new_rules.reverse()\n",
        "        return new_rules\n",
        "\n",
        "    def get_state(self):\n",
        "        state = []\n",
        "        state.extend(to_bits(self.ranges[0], 32))\n",
        "        state.extend(to_bits(self.ranges[1] - 1, 32))\n",
        "        state.extend(to_bits(self.ranges[2], 32))\n",
        "        state.extend(to_bits(self.ranges[3] - 1, 32))\n",
        "        assert len(state) == 128, len(state)\n",
        "        state.extend(to_bits(self.ranges[4], 16))\n",
        "        state.extend(to_bits(self.ranges[5] - 1, 16))\n",
        "        state.extend(to_bits(self.ranges[6], 16))\n",
        "        state.extend(to_bits(self.ranges[7] - 1, 16))\n",
        "        assert len(state) == 192, len(state)\n",
        "        state.extend(to_bits(self.ranges[8], 8))\n",
        "        state.extend(to_bits(self.ranges[9] - 1, 8))\n",
        "        assert len(state) == 208, len(state)\n",
        "\n",
        "        if self.manual_partition is None:\n",
        "            # 0, 6 -> 0-64%\n",
        "            # 6, 7 -> 64-100%\n",
        "            partition_state = [\n",
        "                0,\n",
        "                7,  # [>=min, <max) -- 0%, 2%, 4%, 8%, 16%, 32%, 64%, 100%\n",
        "                0,\n",
        "                7,\n",
        "                0,\n",
        "                7,\n",
        "                0,\n",
        "                7,\n",
        "                0,\n",
        "                7,\n",
        "            ]\n",
        "            for (smaller, part_dim, part_size) in self.partitions:\n",
        "                if smaller:\n",
        "                    partition_state[part_dim * 2 + 1] = min(\n",
        "                        partition_state[part_dim * 2 + 1], part_size + 1)\n",
        "                else:\n",
        "                    partition_state[part_dim * 2] = max(\n",
        "                        partition_state[part_dim * 2], part_size + 1)\n",
        "            state.extend(onehot_encode(partition_state, 7))\n",
        "        else:\n",
        "            partition_state = [0] * 70\n",
        "            partition_state[self.manual_partition] = 1\n",
        "            state.extend(partition_state)\n",
        "        state.append(self.num_rules)\n",
        "        return np.array(state)\n",
        "\n",
        "    def __str__(self):\n",
        "        result = \"ID:%d\\tAction:%s\\tDepth:%d\\tRange:\\t%s\\nChildren: \" % (\n",
        "            self.id, str(self.action), self.depth, str(self.ranges))\n",
        "        for child in self.children:\n",
        "            result += str(child.id) + \" \"\n",
        "        result += \"\\nRules:\\n\"\n",
        "        for rule in self.rules:\n",
        "            result += str(rule) + \"\\n\"\n",
        "        if self.pushup_rules != None:\n",
        "            result += \"Pushup Rules:\\n\"\n",
        "            for rule in self.pushup_rules:\n",
        "                result += str(rule) + \"\\n\"\n",
        "        return result\n",
        "\n",
        "\n",
        "class Tree:\n",
        "    def __init__(\n",
        "            self,\n",
        "            rules,\n",
        "            leaf_threshold,\n",
        "            refinements={\n",
        "                \"node_merging\": False,\n",
        "                \"rule_overlay\": False,\n",
        "                \"region_compaction\": False,\n",
        "                \"rule_pushup\": False,\n",
        "                \"equi_dense\": False\n",
        "            }):\n",
        "        # hyperparameters\n",
        "        self.leaf_threshold = leaf_threshold\n",
        "        self.refinements = refinements\n",
        "\n",
        "        self.rules = rules\n",
        "        self.root = self.create_node(\n",
        "            0, [0, 2**32, 0, 2**32, 0, 2**16, 0, 2**16, 0, 2**8], rules, 1,\n",
        "            None, None)\n",
        "        if (self.refinements[\"region_compaction\"]):\n",
        "            self.refinement_region_compaction(self.root)\n",
        "        self.current_node = self.root\n",
        "        self.nodes_to_cut = [self.root]\n",
        "        self.depth = 1\n",
        "        self.node_count = 1\n",
        "\n",
        "    def create_node(self, id, ranges, rules, depth, partitions,\n",
        "                    manual_partition):\n",
        "        node = Node(id, ranges, rules, depth, partitions, manual_partition)\n",
        "\n",
        "        if self.refinements[\"rule_overlay\"]:\n",
        "            self.refinement_rule_overlay(node)\n",
        "\n",
        "        return node\n",
        "\n",
        "    def match(self, packet):\n",
        "        return self.root.match(packet)\n",
        "\n",
        "    def get_depth(self):\n",
        "        return self.depth\n",
        "\n",
        "    def get_current_node(self):\n",
        "        return self.current_node\n",
        "\n",
        "    def is_leaf(self, node):\n",
        "        return len(node.rules) <= self.leaf_threshold\n",
        "\n",
        "    def is_finish(self):\n",
        "        return len(self.nodes_to_cut) == 0\n",
        "\n",
        "    def update_tree(self, node, children):\n",
        "        if self.refinements[\"node_merging\"]:\n",
        "            children = self.refinement_node_merging(children)\n",
        "\n",
        "        if self.refinements[\"equi_dense\"]:\n",
        "            children = self.refinement_equi_dense(children)\n",
        "\n",
        "        if (self.refinements[\"region_compaction\"]):\n",
        "            for child in children:\n",
        "                self.refinement_region_compaction(child)\n",
        "\n",
        "        node.children.extend(children)\n",
        "        children.reverse()\n",
        "        self.nodes_to_cut.pop()\n",
        "        self.nodes_to_cut.extend(children)\n",
        "        self.current_node = self.nodes_to_cut[-1]\n",
        "\n",
        "    def partition_cutsplit(self):\n",
        "        assert self.current_node is self.root\n",
        "        from cutsplit import CutSplit\n",
        "        self._split(self.root, CutSplit(self.rules), \"cutsplit\")\n",
        "\n",
        "    def partition_efficuts(self):\n",
        "        assert self.current_node is self.root\n",
        "        from efficuts import EffiCuts\n",
        "        self._split(self.root, EffiCuts(self.rules), \"efficuts\")\n",
        "\n",
        "    def _split(self, node, splitter, name):\n",
        "        key = (name, tuple(str(r) for r in self.rules))\n",
        "        if key not in SPLIT_CACHE:\n",
        "            print(\"Split not cached, recomputing\")\n",
        "            SPLIT_CACHE[key] = [\n",
        "                p for p in splitter.separate_rules(self.rules) if len(p) > 0\n",
        "            ]\n",
        "        parts = SPLIT_CACHE[key]\n",
        "\n",
        "        parts.sort(key=lambda x: -len(x))\n",
        "        assert len(self.rules) == sum(len(s) for s in parts)\n",
        "        print(splitter, [len(s) for s in parts])\n",
        "\n",
        "        children = []\n",
        "        for i, p in enumerate(parts):\n",
        "            c = self.create_node(self.node_count, node.ranges, p,\n",
        "                                 node.depth + 1, [], i)\n",
        "            self.node_count += 1\n",
        "            children.append(c)\n",
        "        node.action = (\"partition\", 0, 0)\n",
        "        self.update_tree(node, children)\n",
        "\n",
        "    def partition_current_node(self, part_dim, part_size):\n",
        "        return self.partition_node(self.current_node, part_dimension,\n",
        "                                   part_size)\n",
        "\n",
        "    def partition_node(self, node, part_dim, part_size):\n",
        "        assert part_dim in [0, 1, 2, 3, 4], part_dim\n",
        "        assert part_size in [0, 1, 2, 3, 4, 5], part_size\n",
        "        self.depth = max(self.depth, node.depth + 1)\n",
        "        node.action = (\"partition\", part_dim, part_size)\n",
        "\n",
        "        def fits(rule, threshold):\n",
        "            span = rule.ranges[part_dim * 2 + 1] - rule.ranges[part_dim * 2]\n",
        "            assert span >= 0, rule\n",
        "            return span < threshold\n",
        "\n",
        "        small_rules = []\n",
        "        big_rules = []\n",
        "        max_size = [2**32, 2**32, 2**16, 2**16, 2**8][part_dim]\n",
        "        threshold = max_size * 0.02 * 2**part_size  # 2% ... 64%\n",
        "        for rule in node.rules:\n",
        "            if fits(rule, threshold):\n",
        "                small_rules.append(rule)\n",
        "            else:\n",
        "                big_rules.append(rule)\n",
        "\n",
        "        left_part = list(node.partitions)\n",
        "        left_part.append((True, part_dim, part_size))\n",
        "        left = self.create_node(self.node_count, node.ranges, small_rules,\n",
        "                                node.depth + 1, left_part, None)\n",
        "        self.node_count += 1\n",
        "        right_part = list(node.partitions)\n",
        "        right_part.append((False, part_dim, part_size))\n",
        "        right = self.create_node(self.node_count, node.ranges, big_rules,\n",
        "                                 node.depth + 1, right_part, None)\n",
        "        self.node_count += 1\n",
        "\n",
        "        children = [left, right]\n",
        "        self.update_tree(node, children)\n",
        "        return children\n",
        "\n",
        "    def cut_current_node(self, cut_dimension, cut_num):\n",
        "        return self.cut_node(self.current_node, cut_dimension, cut_num)\n",
        "\n",
        "    def cut_node(self, node, cut_dimension, cut_num):\n",
        "        self.depth = max(self.depth, node.depth + 1)\n",
        "        node.action = (\"cut\", cut_dimension, cut_num)\n",
        "        range_left = node.ranges[cut_dimension * 2]\n",
        "        range_right = node.ranges[cut_dimension * 2 + 1]\n",
        "        range_per_cut = math.ceil((range_right - range_left) / cut_num)\n",
        "\n",
        "        children = []\n",
        "        assert cut_num > 0, (cut_dimension, cut_num)\n",
        "        for i in range(cut_num):\n",
        "            child_ranges = list(node.ranges)\n",
        "            child_ranges[cut_dimension * 2] = range_left + i * range_per_cut\n",
        "            child_ranges[cut_dimension * 2 + 1] = min(\n",
        "                range_right, range_left + (i + 1) * range_per_cut)\n",
        "\n",
        "            child_rules = []\n",
        "            for rule in node.rules:\n",
        "                if rule.is_intersect(cut_dimension,\n",
        "                                     child_ranges[cut_dimension * 2],\n",
        "                                     child_ranges[cut_dimension * 2 + 1]):\n",
        "                    child_rules.append(rule)\n",
        "\n",
        "            child = self.create_node(self.node_count, child_ranges,\n",
        "                                     child_rules, node.depth + 1,\n",
        "                                     node.partitions, node.manual_partition)\n",
        "            children.append(child)\n",
        "            self.node_count += 1\n",
        "\n",
        "        self.update_tree(node, children)\n",
        "        return children\n",
        "\n",
        "    def cut_current_node_multi_dimension(self, cut_dimensions, cut_nums):\n",
        "        self.depth = max(self.depth, self.current_node.depth + 1)\n",
        "        node = self.current_node\n",
        "        node.action = (cut_dimensions, cut_nums)\n",
        "\n",
        "        range_per_cut = []\n",
        "        for i in range(len(cut_dimensions)):\n",
        "            range_left = node.ranges[cut_dimensions[i] * 2]\n",
        "            range_right = node.ranges[cut_dimensions[i] * 2 + 1]\n",
        "            cut_num = cut_nums[i]\n",
        "            range_per_cut.append(\n",
        "                math.ceil((range_right - range_left) / cut_num))\n",
        "\n",
        "        cut_index = [0 for i in range(len(cut_dimensions))]\n",
        "        children = []\n",
        "        while True:\n",
        "            # compute child ranges\n",
        "            child_ranges = list(node.ranges)\n",
        "            for i in range(len(cut_dimensions)):\n",
        "                dimension = cut_dimensions[i]\n",
        "                child_ranges[dimension*2] = node.ranges[dimension*2] + \\\n",
        "                    cut_index[i] * range_per_cut[i]\n",
        "                child_ranges[dimension * 2 + 1] = min(\n",
        "                    node.ranges[dimension * 2 + 1], node.ranges[dimension * 2]\n",
        "                    + (cut_index[i] + 1) * range_per_cut[i])\n",
        "\n",
        "            # compute child rules\n",
        "            child_rules = []\n",
        "            for rule in node.rules:\n",
        "                if rule.is_intersect_multi_dimension(child_ranges):\n",
        "                    child_rules.append(rule)\n",
        "\n",
        "            # create new child\n",
        "            child = self.create_node(self.node_count, child_ranges,\n",
        "                                     child_rules, node.depth + 1,\n",
        "                                     node.partitions, node.manual_partition)\n",
        "            children.append(child)\n",
        "            self.node_count += 1\n",
        "\n",
        "            # update cut index\n",
        "            cut_index[0] += 1\n",
        "            i = 0\n",
        "            while cut_index[i] == cut_nums[i]:\n",
        "                cut_index[i] = 0\n",
        "                i += 1\n",
        "                if i < len(cut_nums):\n",
        "                    cut_index[i] += 1\n",
        "                else:\n",
        "                    break\n",
        "\n",
        "            if i == len(cut_nums):\n",
        "                break\n",
        "\n",
        "        self.update_tree(node, children)\n",
        "        return children\n",
        "\n",
        "    def cut_current_node_split(self, cut_dimension, cut_position):\n",
        "        self.depth = max(self.depth, self.current_node.depth + 1)\n",
        "        node = self.current_node\n",
        "        node.action = (cut_dimension, cut_position)\n",
        "        range_left = node.ranges[cut_dimension * 2]\n",
        "        range_right = node.ranges[cut_dimension * 2 + 1]\n",
        "        range_per_cut = cut_position - range_left\n",
        "\n",
        "        children = []\n",
        "        for i in range(2):\n",
        "            child_ranges = node.ranges.copy()\n",
        "            child_ranges[cut_dimension * 2] = range_left + i * range_per_cut\n",
        "            child_ranges[cut_dimension * 2 + 1] = min(\n",
        "                range_right, range_left + (i + 1) * range_per_cut)\n",
        "\n",
        "            child_rules = []\n",
        "            for rule in node.rules:\n",
        "                if rule.is_intersect(cut_dimension,\n",
        "                                     child_ranges[cut_dimension * 2],\n",
        "                                     child_ranges[cut_dimension * 2 + 1]):\n",
        "                    child_rules.append(rule)\n",
        "\n",
        "            child = self.create_node(self.node_count, child_ranges,\n",
        "                                     child_rules, node.depth + 1,\n",
        "                                     node.partitions, node.manual_partition)\n",
        "            children.append(child)\n",
        "            self.node_count += 1\n",
        "\n",
        "        self.update_tree(node, children)\n",
        "        return children\n",
        "\n",
        "    def get_next_node(self):\n",
        "        self.nodes_to_cut.pop()\n",
        "        if len(self.nodes_to_cut) > 0:\n",
        "            self.current_node = self.nodes_to_cut[-1]\n",
        "        else:\n",
        "            self.current_node = None\n",
        "        return self.current_node\n",
        "\n",
        "    def check_contiguous_region(self, node1, node2):\n",
        "        count = 0\n",
        "        for i in range(5):\n",
        "            if node1.ranges[i*2+1] == node2.ranges[i*2] or \\\n",
        "                    node2.ranges[i*2+1] == node1.ranges[i*2]:\n",
        "                if count == 1:\n",
        "                    return False\n",
        "                else:\n",
        "                    count = 1\n",
        "            elif node1.ranges[i*2] != node2.ranges[i*2] or \\\n",
        "                    node1.ranges[i*2+1] != node2.ranges[i*2+1]:\n",
        "                return False\n",
        "        if count == 0:\n",
        "            return False\n",
        "        return True\n",
        "\n",
        "    def merge_region(self, node1, node2):\n",
        "        for i in range(5):\n",
        "            node1.ranges[i * 2] = min(node1.ranges[i * 2], node2.ranges[i * 2])\n",
        "            node1.ranges[i * 2 + 1] = max(node1.ranges[i * 2 + 1],\n",
        "                                          node2.ranges[i * 2 + 1])\n",
        "\n",
        "    def refinement_node_merging(self, nodes):\n",
        "        while True:\n",
        "            flag = True\n",
        "            merged_nodes = [nodes[0]]\n",
        "            last_node = nodes[0]\n",
        "            for i in range(1, len(nodes)):\n",
        "                if self.check_contiguous_region(last_node, nodes[i]):\n",
        "                    if set(last_node.rules) == set(nodes[i].rules):\n",
        "                        self.merge_region(last_node, nodes[i])\n",
        "                        flag = False\n",
        "                        continue\n",
        "\n",
        "                merged_nodes.append(nodes[i])\n",
        "                last_node = nodes[i]\n",
        "\n",
        "            nodes = merged_nodes\n",
        "            if flag:\n",
        "                break\n",
        "\n",
        "        return nodes\n",
        "\n",
        "    def refinement_rule_overlay(self, node):\n",
        "        if len(node.rules) == 0 or len(node.rules) > 500:\n",
        "            return\n",
        "        node.rules = node.pruned_rules()\n",
        "\n",
        "    def refinement_region_compaction(self, node):\n",
        "        if len(node.rules) == 0:\n",
        "            return\n",
        "\n",
        "        new_ranges = list(node.rules[0].ranges)\n",
        "        for rule in node.rules[1:]:\n",
        "            for i in range(5):\n",
        "                new_ranges[i * 2] = min(new_ranges[i * 2], rule.ranges[i * 2])\n",
        "                new_ranges[i * 2 + 1] = max(new_ranges[i * 2 + 1],\n",
        "                                            rule.ranges[i * 2 + 1])\n",
        "        for i in range(5):\n",
        "            node.ranges[i * 2] = max(new_ranges[i * 2], node.ranges[i * 2])\n",
        "            node.ranges[i * 2 + 1] = min(new_ranges[i * 2 + 1],\n",
        "                                         node.ranges[i * 2 + 1])\n",
        "\n",
        "    def refinement_rule_pushup(self):\n",
        "        nodes_by_layer = [None for i in range(self.depth)]\n",
        "\n",
        "        current_layer_nodes = [self.root]\n",
        "        nodes_by_layer[0] = current_layer_nodes\n",
        "        for i in range(self.depth - 1):\n",
        "            next_layer_nodes = []\n",
        "            for node in current_layer_nodes:\n",
        "                next_layer_nodes.extend(node.children)\n",
        "            nodes_by_layer[i + 1] = next_layer_nodes\n",
        "            current_layer_nodes = next_layer_nodes\n",
        "\n",
        "        for i in reversed(range(self.depth)):\n",
        "            for node in nodes_by_layer[i]:\n",
        "                if len(node.children) == 0:\n",
        "                    node.pushup_rules = set(node.rules)\n",
        "                else:\n",
        "                    node.pushup_rules = set(node.children[0].pushup_rules)\n",
        "                    for j in range(1, len(node.children)):\n",
        "                        node.pushup_rules = node.pushup_rules.intersection(\n",
        "                            node.children[j].pushup_rules)\n",
        "                    for child in node.children:\n",
        "                        child.pushup_rules = child.pushup_rules.difference(\n",
        "                            node.pushup_rules)\n",
        "\n",
        "    def refinement_equi_dense(self, nodes):\n",
        "        # try to merge\n",
        "        nodes_copy = []\n",
        "        max_rule_count = -1\n",
        "        for node in nodes:\n",
        "            nodes_copy.append(\n",
        "                Node(node.id, list(node.ranges), list(node.rules), node.depth,\n",
        "                     node.partitions, node.manual_partition))\n",
        "            max_rule_count = max(max_rule_count, len(node.rules))\n",
        "        while True:\n",
        "            flag = True\n",
        "            merged_nodes = [nodes_copy[0]]\n",
        "            last_node = nodes_copy[0]\n",
        "            for i in range(1, len(nodes_copy)):\n",
        "                if self.check_contiguous_region(last_node, nodes_copy[i]):\n",
        "                    rules = set(last_node.rules).union(\n",
        "                        set(nodes_copy[i].rules))\n",
        "                    if len(rules) < len(last_node.rules) + len(nodes_copy[i].rules) and \\\n",
        "                        len(rules) < max_rule_count:\n",
        "                        rules = list(rules)\n",
        "                        rules.sort(key=lambda i: i.priority)\n",
        "                        last_node.rules = rules\n",
        "                        self.merge_region(last_node, nodes_copy[i])\n",
        "                        flag = False\n",
        "                        continue\n",
        "\n",
        "                merged_nodes.append(nodes_copy[i])\n",
        "                last_node = nodes_copy[i]\n",
        "\n",
        "            nodes_copy = merged_nodes\n",
        "            if flag:\n",
        "                break\n",
        "\n",
        "        # check condition\n",
        "        if len(nodes_copy) <= 8:\n",
        "            nodes = nodes_copy\n",
        "        return nodes\n",
        "\n",
        "    def compute_result(self):\n",
        "        if self.refinements[\"rule_pushup\"]:\n",
        "            self.refinement_rule_pushup()\n",
        "\n",
        "        # memory space\n",
        "        # non-leaf: 2 + 16 + 4 * child num\n",
        "        # leaf: 2 + 16 * rule num\n",
        "        # details:\n",
        "        #     header: 2 bytes\n",
        "        #     region boundary for non-leaf: 16 bytes\n",
        "        #     each child pointer: 4 bytes\n",
        "        #     each rule: 16 bytes\n",
        "        result = {\"bytes_per_rule\": 0, \"memory_access\": 0, \"update_memory_access\": 0,## \\\n",
        "            \"num_leaf_node\": 0, \"num_nonleaf_node\": 0, \"num_node\": 0}\n",
        "        nodes = [self.root]\n",
        "        while len(nodes) != 0:\n",
        "            next_layer_nodes = []\n",
        "            for node in nodes:\n",
        "                next_layer_nodes.extend(node.children)\n",
        "\n",
        "                # compute bytes per rule\n",
        "                if self.is_leaf(node):\n",
        "                    result[\"bytes_per_rule\"] += 2 + 16 * len(node.rules)\n",
        "                    result[\"num_leaf_node\"] += 1\n",
        "                else:\n",
        "                    result[\"bytes_per_rule\"] += 2 + 16 + 4 * len(node.children)\n",
        "                    result[\"num_nonleaf_node\"] += 1\n",
        "\n",
        "            nodes = next_layer_nodes\n",
        "\n",
        "        result[\"memory_access\"] = self._compute_memory_access(self.root)\n",
        "        result[\"update_memory_access\"] = self._compute_update_memory_access(self.root)\n",
        "        result[\"bytes_per_rule\"] = result[\"bytes_per_rule\"] / len(self.rules)\n",
        "        result[\n",
        "            \"num_node\"] = result[\"num_leaf_node\"] + result[\"num_nonleaf_node\"]\n",
        "        return result\n",
        "\n",
        "    def _compute_update_memory_access(self, node):\n",
        "        if self.is_leaf(node) or not node.children:\n",
        "            return 1\n",
        "\n",
        "        if node.is_partition():\n",
        "            return 1 + max(\n",
        "                self._compute_update_memory_access(n) for n in node.children)\n",
        "        else:\n",
        "            return sum(self._compute_update_memory_access(n) for n in node.children)\n",
        "\n",
        "\n",
        "    def _compute_memory_access(self, node):\n",
        "        if self.is_leaf(node) or not node.children:\n",
        "            return 1\n",
        "\n",
        "        if node.is_partition():\n",
        "            return sum(self._compute_memory_access(n) for n in node.children)\n",
        "        else:\n",
        "            return 1 + max(\n",
        "                self._compute_memory_access(n) for n in node.children)\n",
        "\n",
        "    def get_stats(self):\n",
        "        widths = []\n",
        "        dim_stats = []\n",
        "        nodes = [self.root]\n",
        "        while len(nodes) != 0 and len(widths) < 30:\n",
        "            dim = [0] * 5\n",
        "            next_layer_nodes = []\n",
        "            for node in nodes:\n",
        "                next_layer_nodes.extend(node.children)\n",
        "                if node.action and node.action[0] == \"cut\":\n",
        "                    dim[node.action[1]] += 1\n",
        "            widths.append(len(nodes))\n",
        "            dim_stats.append(dim)\n",
        "            nodes = next_layer_nodes\n",
        "        return {\n",
        "            \"widths\": widths,\n",
        "            \"dim_stats\": dim_stats,\n",
        "        }\n",
        "\n",
        "    def stats_str(self):\n",
        "        stats = self.get_stats()\n",
        "        out = \"widths\" + \",\" + \",\".join(map(str, stats[\"widths\"]))\n",
        "        out += \"\\n\"\n",
        "        for i in range(len(stats[\"dim_stats\"][0])):\n",
        "            out += \"dim{}\".format(i) + \",\" + \",\".join(\n",
        "                str(d[i]) for d in stats[\"dim_stats\"])\n",
        "            out += \"\\n\"\n",
        "        return out\n",
        "\n",
        "    def print_stats(self):\n",
        "        print(self.stats_str())\n",
        "\n",
        "    def print_layers(self, layer_num=5):\n",
        "        path = '/content/drive/MyDrive/CIAL/output.txt'\n",
        "        f = open(path, 'w')\n",
        "        nodes = [self.root]\n",
        "        for i in range(layer_num):\n",
        "            if len(nodes) == 0:\n",
        "                return\n",
        "            print(\"Layer\", i, file=f)\n",
        "            next_layer_nodes = []\n",
        "            for node in nodes:\n",
        "                print(node, file=f)                \n",
        "                next_layer_nodes.extend(node.children)\n",
        "            nodes = next_layer_nodes\n",
        "\n",
        "    def __str__(self):\n",
        "        result = \"\"\n",
        "        nodes = [self.root]\n",
        "        while len(nodes) != 0:\n",
        "            next_layer_nodes = []\n",
        "            for node in nodes:\n",
        "                result += \"%d; %s; %s; [\" % (node.id, str(node.action),\n",
        "                                             str(node.ranges))\n",
        "                for child in node.children:\n",
        "                    result += str(child.id) + \" \"\n",
        "                result += \"]\\n\"\n",
        "                next_layer_nodes.extend(node.children)\n",
        "            nodes = next_layer_nodes\n",
        "        return result\n",
        "```\n"
      ],
      "metadata": {
        "id": "3GISoJUKC97Q"
      }
    }
  ]
}